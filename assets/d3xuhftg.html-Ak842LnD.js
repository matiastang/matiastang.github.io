import{_ as a,c as n,b as t,o as r}from"./app-CS9K37Kg.js";const o={};function l(s,e){return r(),n("div",null,e[0]||(e[0]=[t("p",null,"Transformer 在长序列上速度慢且内存消耗巨大，因为自我注意的时间和记忆复杂度在序列长度上是二次方的。近似注意力方法试图通过牺牲模型质量来降低计算复杂性来解决这个问题，但通常无法实现挂钟加速。我们认为，一个缺失的原则是使注意力算法具有 IO 感知能力 —— 考虑 GPU 内存级别之间的读取和写入。我们提出了 FlashAttention，这是一种 IO 感知的精确注意力算法，它使用平铺来减少 GPU 高带宽内存 （HBM） 和 GPU 片上 SRAM 之间的内存读/写次数。我们分析了 FlashAttention 的 IO 复杂性，表明它比标准注意需要更少的 HBM 访问，并且是一系列 SRAM 大小的最佳选择。我们还将 FlashAttention 扩展到块稀疏注意力，从而产生一种比任何现有的近似注意力方法都快的近似注意力算法。FlashAttention 训练 Transformer 的速度比现有基线更快：与 MLPerf 1.1 训练速度记录相比，BERT-large 的端到端挂钟加速 15%（序列长度 512），GPT-2 的 × 加速 3（序列长度 1K），远程竞技场的 × 加速 2.4（序列长度 1K-4K）。FlashAttention 和块稀疏 FlashAttention 可在 Transformer 中实现更长的上下文，从而产生更高质量的模型（GPT-2 的困惑度提高 0.7 点，长文档分类的提升 6.4 个点）和全新的功能：第一个在 Path-X 挑战（序列长度 16K，准确率 61.4%）和 Path-256（序列长度 64K，准确率 63.1%）上实现优于机会性能的 Transformer。",-1),t("h2",{id:"参考",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#参考"},[t("span",null,"参考")])],-1),t("p",null,[t("a",{href:"https://arxiv.org/abs/2205.14135",target:"_blank",rel:"noopener noreferrer"},"《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》")],-1),t("p",null,[t("a",{href:"https://github.com/Dao-AILab/flash-attention",target:"_blank",rel:"noopener noreferrer"},"GitHub FlashAttention")],-1)]))}const h=a(o,[["render",l],["__file","d3xuhftg.html.vue"]]),m=JSON.parse('{"path":"/llm/attention/d3xuhftg.html","title":"Flash Attention","lang":"zh-CN","frontmatter":{"title":"Flash Attention","createTime":"2025/03/21 17:49:04","permalink":"/llm/attention/d3xuhftg.html","watermark":true},"headers":[],"readingTime":{"minutes":1.59,"words":478},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/attention/FlashAttention.md","bulletin":false}');export{h as comp,m as data};
