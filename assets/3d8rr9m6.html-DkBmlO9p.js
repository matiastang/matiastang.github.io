import{_ as t,c as a,b as e,o as l}from"./app-CS9K37Kg.js";const o={};function s(n,r){return l(),a("div",null,r[0]||(r[0]=[e("p",null,"神经网络吸收信息的能力受其参数数量的限制。条件计算，即网络的各个部分在每个样本的基础上处于活动状态，在理论上已被提出作为一种在不按比例增加计算量的情况下显着增加模型容量的方法。然而，在实践中，存在重大的算法和性能挑战。在这项工作中，我们解决了这些挑战，并最终实现了条件计算的承诺，在现代 GPU 集群上实现了超过 1000 倍的模型容量改进，而计算效率仅损失很小。我们引入了一个稀疏门控专家混合层 （MoE），由多达数千个前馈子网络组成。可训练的门控网络确定这些专家的稀疏组合，用于每个示例。我们将 MoE 应用于语言建模和机器翻译的任务，其中模型能力对于吸收训练语料库中可用的大量知识至关重要。我们提出了一些模型架构，其中具有多达 1370 亿个参数的 MoE 在堆叠的 LSTM 层之间以卷积方式应用。在大型语言建模和机器翻译基准测试中，这些模型以较低的计算成本获得的结果明显优于最先进的模型。",-1),e("h2",{id:"参考",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#参考"},[e("span",null,"参考")])],-1),e("p",null,[e("a",{href:"https://arxiv.org/abs/1701.06538",target:"_blank",rel:"noopener noreferrer"},"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer")],-1)]))}const i=t(o,[["render",s],["__file","3d8rr9m6.html.vue"]]),d=JSON.parse('{"path":"/llm/3d8rr9m6.html","title":"The Sparsely-Gated Mixture-of-Experts Layer","lang":"zh-CN","frontmatter":{"title":"The Sparsely-Gated Mixture-of-Experts Layer","createTime":"2025/02/22 10:22:16","permalink":"/llm/3d8rr9m6.html","watermark":true},"headers":[],"readingTime":{"minutes":1.34,"words":402},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/MOE.md","bulletin":false}');export{i as comp,d as data};
