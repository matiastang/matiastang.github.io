import{_ as t,c as r,a,o}from"./app-CS9K37Kg.js";const n="/notes/llm/google/Transformer%E6%9E%B6%E6%9E%84.png",l={};function s(m,e){return o(),r("div",null,e[0]||(e[0]=[a('<p>Transformer 是一种避免使用循环的模型架构，完全依赖注意力机制来描述输入和输出之间的全局依赖关系。</p><p><img src="'+n+'" alt="Transformer架构"></p><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考</span></a></h2><p><a href="https://arxiv.org/html/1706.03762v7" target="_blank" rel="noopener noreferrer">Attention Is All You Need 第7版本</a></p><p><a href="https://zhuanlan.zhihu.com/p/682007654" target="_blank" rel="noopener noreferrer">Attention is All you Need 全文翻译</a></p>',5)]))}const p=t(l,[["render",s],["__file","0tiv1eo5.html.vue"]]),c=JSON.parse('{"path":"/llm/google/0tiv1eo5.html","title":"Transformer","lang":"zh-CN","frontmatter":{"title":"Transformer","createTime":"2025/02/19 18:06:53","permalink":"/llm/google/0tiv1eo5.html","watermark":true},"headers":[],"readingTime":{"minutes":0.34,"words":103},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/google/Transformer.md","bulletin":false}');export{p as comp,c as data};
