import{_ as a,c as n,a as t,o as r}from"./app-CS9K37Kg.js";const i={};function l(p,e){return r(),n("div",null,e[0]||(e[0]=[t('<p><code>LLM</code>（<code>Large Language Model</code>）的发展历程：</p><h2 id="_2017年" tabindex="-1"><a class="header-anchor" href="#_2017年"><span>2017年</span></a></h2><ul><li>Google发布Transformer架构</li></ul><p><a href="https://arxiv.org/html/1706.03762v7" target="_blank" rel="noopener noreferrer">Attention Is All You Need 第7版本</a></p><div class="hint-container note"><p class="hint-container-title">注</p><p>Transformer是一个革命性的框架，后面的BERT、GPT等模型都是基于它构建的。</p></div><h2 id="_2018年" tabindex="-1"><a class="header-anchor" href="#_2018年"><span>2018年</span></a></h2><ul><li>BERT和GPT模型发布</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>BERT和GPT模型在自然语言处理领域取得了重大突破，提高了上下文理解和文本生成能力。</p></div><h2 id="_2019年" tabindex="-1"><a class="header-anchor" href="#_2019年"><span>2019年</span></a></h2><ul><li>GPT-2发布</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>GPT-2拥有15亿参数，展示了卓越的少样本和零样本学习能力，但同时也存在幻觉问题。</p></div><h2 id="_2020年" tabindex="-1"><a class="header-anchor" href="#_2020年"><span>2020年</span></a></h2><ul><li>GPT-3发布</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>GPT-3拥有1750亿参数，展示了卓越的少样本和零样本学习能力，但同时也存在幻觉问题。</p></div><h2 id="_2021年" tabindex="-1"><a class="header-anchor" href="#_2021年"><span>2021年</span></a></h2><ul><li>GPT-3.5发布</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>GPT-3.5通过引入「对话式」交互，采用监督微调和基于人类反馈的强化学习，解决了GPT-3的幻觉问题。</p></div><h2 id="_2022年" tabindex="-1"><a class="header-anchor" href="#_2022年"><span>2022年</span></a></h2><ul><li>ChatGPT发布</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>ChatGPT通过对话式交互，采用监督微调和基于人类反馈的强化学习，解决了GPT-3的幻觉问题。</p></div><h2 id="_2023年" tabindex="-1"><a class="header-anchor" href="#_2023年"><span>2023年</span></a></h2><ul><li>GPT-4发布</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>GPT-4整合了文本、图像和音频处理能力，使LLM能够以更接近人类的听、说、看能力。</p></div><h2 id="_2024年" tabindex="-1"><a class="header-anchor" href="#_2024年"><span>2024年</span></a></h2><ul><li>OpenAI-o1发布</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>OpenAI-o1是一个推理模型，在复杂问题解决方面取得突破，赋予了LLM更接近人类系统2思维的深度推理能力，并挑战了AI领域的传统规范。</p></div><h2 id="_2025年" tabindex="-1"><a class="header-anchor" href="#_2025年"><span>2025年</span></a></h2><ul><li>DeepSeek-R1发布</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>DeepSeek-R1是一个推理模型，在复杂问题解决方面取得突破，赋予了LLM更接近人类系统2思维的深度推理能力，并挑战了AI领域的传统规范。</p></div><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>2025年初，中国推出了具有开创性且高性价比的「大型语言模型」（Large Language Model — LLM）DeepSeek-R1，引发了AI的巨大变革。本文回顾了LLM的发展历程，</p><p>2017年革命性的Transformer架构，该架构通过「自注意力机制」(Self-Attention)彻底重塑了自然语言处理。</p><p>2018年，BERT和GPT等模型崭露头角，显著提升了上下文理解和文本生成能力。</p><p>2020年，拥有1750亿参数的GPT-3展示了卓越的「少样本」和「零样本」学习能力。然而，「幻觉」问题 — —即生成内容与事实不符，甚至出现「一本正经地胡说八道」的现象 — — 成为了一个关键挑战。</p><p>2022年，OpenAI通过开发「对话式」的ChatGPT应对这一问题，采用了「监督微调」（SFT）和「基于人类反馈的强化学习」（RLHF）。</p><p>2023年，像GPT-4这样的「多模态模型」整合了文本、图像和音频处理能力，使LLM能够以更接近人类的「听」、「说」、「看」能力。</p><p>近期推出的OpenAI-o1和DeepSeek-R1「推理模型」(Reasoning Model)在复杂问题解决方面取得突破，赋予LLM更接近人类「系统2思维」的深度推理能力，标志着人工智能在模拟人类思维模式上迈出了重要一步。</p><p>此外，DeepSeek-R1模型以其「超成本效益」和「开源」设计挑战了AI领域的传统规范，推动了先进LLL的普及，并促进了各行业的创新。</p><p><a href="https://arxiv.org/abs/2502.11089" target="_blank" rel="noopener noreferrer">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a></p><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考</span></a></h2><p><a href="https://arxiv.org/abs/2502.11089" target="_blank" rel="noopener noreferrer">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a></p><p><a href="https://mp.weixin.qq.com/s/nIX2HxfLjeHqBlCqFiRSOA" target="_blank" rel="noopener noreferrer">LLMs发展史：从Transformer（2017）到DeepSeek-R1（2025）</a></p><p><a href="https://mp.weixin.qq.com/s/zUfUVZ3Pn5YbaG1hUS0foQ" target="_blank" rel="noopener noreferrer">大语言模型简史：从Transformer（2017）到DeepSeek-R1（2025）的进化之路</a></p>',43)]))}const o=a(i,[["render",l],["__file","laapcq9c.html.vue"]]),h=JSON.parse('{"path":"/llm/laapcq9c.html","title":"LLM 发展史","lang":"zh-CN","frontmatter":{"title":"LLM 发展史","createTime":"2025/02/19 10:22:35","permalink":"/llm/laapcq9c.html","watermark":true},"headers":[],"readingTime":{"minutes":3.08,"words":924},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/LLM发展史.md","bulletin":false}');export{o as comp,h as data};
