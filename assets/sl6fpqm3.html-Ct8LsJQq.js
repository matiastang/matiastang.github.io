import{_ as o,c as a,b as e,o as l}from"./app-CS9K37Kg.js";const r={};function n(s,t){return l(),a("div",null,t[0]||(t[0]=[e("h1",{id:"模型导出",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#模型导出"},[e("span",null,"模型导出")])],-1),e("p",null,'当我们使用深度学习模型进行实际生产过程中的服务时，很多时候不会直接使用原始的模型文件（比如，PyTorch的pytorch_model.bin等需要通过load_state_dict加载的二进制文件），这是因为这些模型文件只能在python环境中运行，而且无法进行"优化"。举个例子，假如某个环境是嵌入式设备的单片机，那么在它的上面安装python环境是非常费力的；另外如果模型比较大，想要达到比较高的QPS，模型需要摆脱python环境而使用更快的C++库，或进行一些硬件相关的优化，或进行一些算子的融合。',-1),e("p",null,'在这种情况下，使用原始的代码和二进制文件运行就比较得不偿失，因此各类算法库都提供了对应的"导出格式"，比如PyTorch的TorchScript，TensorFlow的GraphDef，或者跨框架格式ONNX。这些格式不仅包含了模型的各类参数，也包含了模型动态图本身，因此可以脱离python环境独立运行并可以获得一定的运行加速，不少算子库也支持以这些格式为起点进行后续优化。ModelScope提供了其内模型的导出方法，用户可以自由选用。',-1),e("p",null,"ONNX 全称为开放神经网络交换格式（Open Neural Network Exchange），是微软和Facebook（Meta）联合提出用于表示深度学习模型的文件格式。 其特点为标准的文件格式，且具备平台无关性。也就是说，用户在任意框架（TensorFlow/PyTorch/JAX等）中训练得到的原始模型都可以转换为这种格式进行存储和优化，或转换为其他框架专用的模型文件。ONNX文件和其他输出格式一样，不仅存储了模型权重，也存储了模型DAG图以及一些有用的辅助信息。",-1)]))}const c=o(r,[["render",n],["__file","sl6fpqm3.html.vue"]]),i=JSON.parse('{"path":"/llm/models/sl6fpqm3.html","title":"模型导出","lang":"zh-CN","frontmatter":{"title":"模型导出","createTime":"2025/01/20 18:02:53","permalink":"/llm/models/sl6fpqm3.html","watermark":true},"headers":[],"readingTime":{"minutes":1.81,"words":542},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/models/模型导出.md","bulletin":false}');export{c as comp,i as data};
