import{_ as a,c as t,a as r,o as l}from"./app-CS9K37Kg.js";const i={};function n(o,e){return l(),t("div",null,e[0]||(e[0]=[r('<h1 id="llm推理服务框架" tabindex="-1"><a class="header-anchor" href="#llm推理服务框架"><span>LLM推理服务框架</span></a></h1><h2 id="llm推理有很多框架-各有其特点-下面分别介绍一下表中七个框架的关键点" tabindex="-1"><a class="header-anchor" href="#llm推理有很多框架-各有其特点-下面分别介绍一下表中七个框架的关键点"><span>LLM推理有很多框架，各有其特点，下面分别介绍一下表中七个框架的关键点：</span></a></h2><ul><li>vLLM[1]：适用于大批量Prompt输入，并对推理速度要求高的场景；</li><li>Text generation inference[2]：依赖HuggingFace模型，并且不需要为核心模型增加多个adapter的场景；</li><li>CTranslate2[3]：可在CPU上进行推理；</li><li>OpenLLM[4]：为核心模型添加adapter并使用HuggingFace Agents，尤其是不完全依赖PyTorch；</li><li>Ray Serve[5]：稳定的Pipeline和灵活的部署，它最适合更成熟的项目；</li><li>MLC LLM[6]：可在客户端（边缘计算）（例如，在Android或iPhone平台上）本地部署LLM；</li><li>DeepSpeed-MII[7]：使用DeepSpeed库来部署LLM；</li></ul><p><a href="https://zhuanlan.zhihu.com/p/653352979" target="_blank" rel="noopener noreferrer">LLM推理部署（一）</a></p>',4)]))}const c=a(i,[["render",n],["__file","o883rut3.html.vue"]]),m=JSON.parse('{"path":"/article/o883rut3.html","title":"LLM推理服务框架","lang":"zh-CN","frontmatter":{"title":"LLM推理服务框架","createTime":"2025/01/20 18:03:30","permalink":"/article/o883rut3.html","watermark":true},"headers":[],"readingTime":{"minutes":0.8,"words":240},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/LLM推理服务框架.md","categoryList":[{"id":"4358b5","sort":10000,"name":"notes"},{"id":"40c37c","sort":10039,"name":"llm"}],"bulletin":false}');export{c as comp,m as data};
