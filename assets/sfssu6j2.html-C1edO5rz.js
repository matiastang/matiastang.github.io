import{_ as n,c as s,a as l,b as a,d,e as r,f as o,r as m,o as h}from"./app-CS9K37Kg.js";const i={};function p(c,t){const e=m("RouteLink");return h(),s("div",null,[t[3]||(t[3]=l('<p>本地部署大型语言模型（LLM）的方式有很多种，可以根据自己的需求选择合适的方案。以下是一些常见的本地部署 LLM 的方法：</p><table><thead><tr><th>方案</th><th>特点</th></tr></thead><tbody><tr><td>Ollama</td><td>简单易用、快速运行</td></tr><tr><td>LM Studio</td><td>可视化管理、非技术用户</td></tr><tr><td>vLLM</td><td>高性能推理、服务器端部署</td></tr><tr><td>llama.cpp</td><td>CPU 设备、本地轻量运行</td></tr><tr><td>text-generation-webui</td><td>大模型微调、本地 Web UI</td></tr><tr><td>GPTQ / AWQ</td><td>低显存 GPU（8GB）运行</td></tr></tbody></table><p>根据你的需求和资源，选择最适合你的方案。</p><h2 id="ollama" tabindex="-1"><a class="header-anchor" href="#ollama"><span>Ollama</span></a></h2><p><code>Ollama</code> 是一个专注于本地化部署和运行大型语言模型（<code>LLM</code>）的工具，旨在让用户能够在自己的设备上高效地运行和微调模型。</p>',5)),a("p",null,[t[1]||(t[1]=d("具体使用可以参考")),r(e,{to:"/llm/ollama/oodx3ekp.html"},{default:o(()=>t[0]||(t[0]=[d("Ollama 使用")])),_:1})]),t[4]||(t[4]=l('<h2 id="lm-studio" tabindex="-1"><a class="header-anchor" href="#lm-studio"><span>LM Studio</span></a></h2><h2 id="vllm" tabindex="-1"><a class="header-anchor" href="#vllm"><span>vLLM</span></a></h2><h2 id="llama-cpp" tabindex="-1"><a class="header-anchor" href="#llama-cpp"><span>llama.cpp</span></a></h2><h2 id="text-generation-webui" tabindex="-1"><a class="header-anchor" href="#text-generation-webui"><span>text-generation-webui</span></a></h2><h2 id="gptq-awq" tabindex="-1"><a class="header-anchor" href="#gptq-awq"><span><code>GPTQ / AWQ</code></span></a></h2><h2 id="ollama、lm-studio、vllm的区别" tabindex="-1"><a class="header-anchor" href="#ollama、lm-studio、vllm的区别"><span>Ollama、LM Studio、vLLM的区别</span></a></h2><table><thead><tr><th>特性</th><th>Ollama</th><th>LM Studio</th><th>vLLM</th></tr></thead><tbody><tr><td>定位</td><td>轻量级、易用的本地 LLM 运行环境</td><td>GUI 友好的本地 LLM 运行器</td><td>高性能推理引擎</td></tr><tr><td>主要特点</td><td>命令行工具，支持快速下载和运行模型，管理模型缓存</td><td>可视化界面，支持 OpenAI API 兼容，便于选择和运行模型</td><td>高吞吐量推理，专注高效并发请求</td></tr><tr><td>适用人群</td><td>开发者、终端用户，想要快速使用本地 LLM</td><td>普通用户，需要 GUI 方便管理模型</td><td>需要高性能推理的开发者（如服务器端推理）</td></tr><tr><td>支持模型</td><td>Mistral, Llama, Gemma, Phi, Code Llama, etc.</td><td>Llama, Mistral, Gemma, Code Llama, etc.</td><td>Hugging Face Transformers 生态模型</td></tr><tr><td>API 兼容性</td><td>提供 OpenAI 兼容 API（可用于 ChatGPT 替代）</td><td>提供 OpenAI 兼容 API，可本地化部署</td><td>提供 OpenAI 兼容 API，适用于高并发</td></tr><tr><td>性能优化</td><td>内置 GPU 加速、量化优化</td><td>主要依赖 GGUF 格式模型，支持 GPU 加速</td><td>专为 GPU 优化，支持 PagedAttention，提高推理速度</td></tr><tr><td>使用方式</td><td>终端命令 (ollama run llama3)</td><td>图形界面，可视化管理模型</td><td>代码库，需要 Python 代码调用</td></tr><tr><td>部署难度</td><td>简单，开箱即用</td><td>非常简单</td><td>适用于开发者，需代码集成</td></tr><tr><td>GPU 加速</td><td>是（支持 CUDA/Metal）</td><td>是（支持 GPU 推理）</td><td>是（深度优化）</td></tr><tr><td>社区支持</td><td>官方 + 开源社区</td><td>官方 + 开源社区</td><td>由开发者使用</td></tr></tbody></table><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>如果你需要本地运行 AI 聊天，<code>Ollama / LM Studio</code> 是最简单的方案。如果你要在服务器上高效推理 <code>vLLM</code> 是最佳选择。对于超低端设备 <code>llama.cpp</code> 是不错的选择。</p><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考</span></a></h2>',10)),a("p",null,[r(e,{to:"/llm/ollama/oodx3ekp.html"},{default:o(()=>t[2]||(t[2]=[d("Ollama 使用")])),_:1})]),t[5]||(t[5]=a("p",null,[a("a",{href:"https://ollama.com",target:"_blank",rel:"noopener noreferrer"},"Ollama")],-1))])}const L=n(i,[["render",p],["__file","sfssu6j2.html.vue"]]),b=JSON.parse('{"path":"/llm/sfssu6j2.html","title":"本地大模型部署","lang":"zh-CN","frontmatter":{"title":"本地大模型部署","createTime":"2025/02/13 10:50:18","permalink":"/llm/sfssu6j2.html","watermark":true},"headers":[],"readingTime":{"minutes":2.28,"words":685},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/本地大模型部署.md","bulletin":false}');export{L as comp,b as data};
