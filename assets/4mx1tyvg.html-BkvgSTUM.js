import{_ as r,c as a,a as t,o as n}from"./app-CS9K37Kg.js";const o={};function l(s,e){return n(),a("div",null,e[0]||(e[0]=[t('<h1 id="llm" tabindex="-1"><a class="header-anchor" href="#llm"><span>LLM</span></a></h1><h2 id="大模型" tabindex="-1"><a class="header-anchor" href="#大模型"><span>大模型</span></a></h2><h3 id="百度千帆" tabindex="-1"><a class="header-anchor" href="#百度千帆"><span>百度千帆</span></a></h3><h3 id="百川" tabindex="-1"><a class="header-anchor" href="#百川"><span>百川</span></a></h3><h3 id="智谱ai" tabindex="-1"><a class="header-anchor" href="#智谱ai"><span>智谱AI</span></a></h3><h2 id="向量数据库" tabindex="-1"><a class="header-anchor" href="#向量数据库"><span>向量数据库</span></a></h2><h2 id="大模型应用开" tabindex="-1"><a class="header-anchor" href="#大模型应用开"><span>大模型应用开</span></a></h2><ul><li><p>Prompt</p></li><li><p>Embeddings</p></li><li><p>Fine-tune</p></li><li><p>SQL</p></li><li><p>Function call</p></li><li><p>RAG</p></li><li><p>Agent</p></li><li><p>Langchain</p></li><li><p>Autogen</p></li><li><p>memGPT</p></li></ul><h2 id="社区" tabindex="-1"><a class="header-anchor" href="#社区"><span>社区</span></a></h2><p><a href="https://modelscope.cn/my/overview" target="_blank" rel="noopener noreferrer">modelscope</a></p><p><a href="https://zhuanlan.zhihu.com/p/648254009" target="_blank" rel="noopener noreferrer">扩散模型</a></p><p><a href="https://python.langchain.com/v0.1/docs/get_started/quickstart/" target="_blank" rel="noopener noreferrer">langchain quickstart</a></p><p><a href="https://platform.openai.com/docs/api-reference/" target="_blank" rel="noopener noreferrer">OpenAI API</a></p><p><a href="https://blog.csdn.net/v_JULY_v/article/details/133619540" target="_blank" rel="noopener noreferrer">通透理解FlashAttention(含其2代和第3代)：全面降低显存读写、加快计算速度</a></p><p><a href="https://blog.csdn.net/v_JULY_v/article/details/128579457" target="_blank" rel="noopener noreferrer">ChatGPT技术原理解析：从RL之PPO算法、RLHF到GPT4、instructGPT</a></p><p><a href="https://blog.csdn.net/v_JULY_v/article/details/128579457" target="_blank" rel="noopener noreferrer">ChatGPT通俗导论：从RL之PPO算法、RLHF到GPT-N、instructGPT</a></p><h1 id="大模型相关" tabindex="-1"><a class="header-anchor" href="#大模型相关"><span>大模型相关</span></a></h1><p>NNLM → Word2Vec → Seq2Seq → Seq2Seq with Attention → Transformer → Elmo → GPT(关于GPT，可再重点看下这篇ChatGPT技术原理解析) → BERT</p><p><a href="Transformer%E9%80%9A%E4%BF%97%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%BB%8EWord2Vec%E3%80%81Seq2Seq%E9%80%90%E6%AD%A5%E7%90%86%E8%A7%A3%E5%88%B0GPT%E3%80%81BERT">Transformer</a><a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener noreferrer">Jay Alammar 的 word2vec</a><a href="https://blog.csdn.net/v_JULY_v/article/details/89894058" target="_blank" rel="noopener noreferrer">如何从RNN起步，一步一步通俗理解LSTM</a><a href="https://jalammar.github.io/illustrated-transformer" target="_blank" rel="noopener noreferrer">一篇图解Transformer：The Illustrated Transformer</a><a href="https://www.julyedu.com/questions/interview-detail?kp_id=30&amp;cate=NLP&amp;quesId=3008" target="_blank" rel="noopener noreferrer">《说说NLP中的预训练技术发展史：从Word Embedding到Bert模型》</a></p><p><a href="https://blog.csdn.net/v_JULY_v/article/details/128579457" target="_blank" rel="noopener noreferrer">ChatGPT技术原理解析：从RL之PPO算法、RLHF到GPT4、instructGPT</a><a href="https://blog.csdn.net/v_JULY_v/article/details/128579457" target="_blank" rel="noopener noreferrer">ChatGPT通俗导论：从RL之PPO算法、RLHF到GPT-N、instructGPT</a></p><h2 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h2><p><a href="https://blog.csdn.net/v_JULY_v/article/details/127411638" target="_blank" rel="noopener noreferrer">Transformer通俗笔记：从Word2Vec、Seq2Seq逐步理解到GPT、BERT</a>的参考。</p><p>国外一牛人Jay Alammar写的 图解Word2Vec（如果你打不开英文原文，可看此翻译版）.. Encoder-Decoder 和 Seq2Seq 《如何从RNN起步，一步一步通俗理解LSTM》，July 《深度学习中的注意力机制(2017版)》，张俊林 Transformer原始论文：Attention Is All You Need，相信读完本文再读原始论文就没啥问题了，另 这是李沐的解读 还是Jay Alammar写的图解transformer（如果打不开英文原文，可看：翻译版1、翻译版2）</p><p>a_journey_into_math_of_ml/03_transformer_tutorial_1st_part，这篇关于Transformer的文章，启发我细究权重矩阵如何而来 《说说NLP中的预训练技术发展史：从Word Embedding到Bert模型》，张俊林</p><p>深度学习：前沿技术-从Attention,Transformer,ELMO,GPT到BERT</p><p>自然语言处理中的Transformer和BERT</p><p>超细节的BERT/Transformer知识点</p><p>《The Illustrated GPT-2 (Visualizing Transformer Language Models)》（翻译版1 翻译版2）</p><p>Transformer结构及其应用详解--GPT、BERT、MT-DNN、GPT-2</p><p>BERT原始论文及与一翻译版本的对照阅读，注意翻译有些小问题，所以建议对照阅读</p><p>NLP陈博士：从BERT原始论文看BERT的原理及实现</p><p>NLP陈博士：Transformer通用特征提取器，和上面这两个公开课值得仔细学习</p><p>CNN笔记：通俗理解卷积神经网络，July</p><p>如何通俗理解Word2Vec</p><p>如何理解反向传播算法BackPropagation 词向量经典模型：从word2vec、glove、ELMo到BERT 《预训练语言模型》，电子工业出版社 【理论篇】是时候彻底弄懂BERT模型了(收藏) 迁移学习和Fine-tuning的一个小课 如何评价 BERT 模型？ 《Attention is All You Need》浅读（简介+代码） Transformer的编码实现：The Annotated Transformer 动手学深度学习 9.2节微调 Transformer论文逐段精读 transformer架构的核心公式其实类似于数学期望，理解起来也不复杂，但为什么这个模型这么强呢？ 台大李宏毅自注意力机制和Transformer详解(这是其PDF课件：self-attention、Transformer，这是其视频笔记：李宏毅深度学习课程笔记（一）——Self-attention和Transformer） 从零实现Transformer的简易版与强大版：从300多行到3000多行</p><h2 id="优秀作者" tabindex="-1"><a class="header-anchor" href="#优秀作者"><span>优秀作者</span></a></h2><p><a href="https://blog.csdn.net/v_JULY_v?type=blog" target="_blank" rel="noopener noreferrer">v_JULY_v</a><a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener noreferrer">Jay Alammar word2vec</a></p>',37)]))}const i=r(o,[["render",l],["__file","4mx1tyvg.html.vue"]]),h=JSON.parse('{"path":"/llm/4mx1tyvg.html","title":"LLM","lang":"zh-CN","frontmatter":{"title":"LLM","createTime":"2025/01/20 17:35:16","permalink":"/llm/4mx1tyvg.html","watermark":true},"headers":[],"readingTime":{"minutes":3.01,"words":904},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/概览.md","bulletin":false}');export{i as comp,h as data};
