import{_ as a,a as n,b as t,c as p,d as e,e as m,f as l,g as i}from"./训练速度-Tua2KsFb.js";import{_ as r,c as h,a as c,o}from"./app-CS9K37Kg.js";const g={};function d(u,s){return o(),h("div",null,s[0]||(s[0]=[c('<p>2025年2月16日，DeepSeek 公布了论文 <a href="https://arxiv.org/abs/2502.11089" target="_blank" rel="noopener noreferrer">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a>。</p><p>NSA（Native Sparse Attention）是一种原生可训练的稀疏注意力机制，它运用创新算法与实现硬件对齐，以实现高效的长上下文建模。</p><h2 id="nsa-原理" tabindex="-1"><a class="header-anchor" href="#nsa-原理"><span>NSA 原理</span></a></h2><p>NSA 采用动态分层稀疏策略，将粗粒度令牌压缩与细粒度令牌选择相结合，以保持全局上下文感知和局部精度。通过下面两项关键创新推进了稀疏注意力设计：</p><ol><li>通过算术强度平衡算法设计实现了大幅加速，并针对现代硬件进行了优化。</li><li>支持端到端训练，在不牺牲模型性能的情况下减少训练计算。</li></ol><h2 id="nsa-vs-full-attention" tabindex="-1"><a class="header-anchor" href="#nsa-vs-full-attention"><span>NSA vs Full Attention</span></a></h2><p><img src="'+a+'" alt="NASvsFA"></p><p>论文中 Full Attention 模型与 NSA 的性能和效率对比。</p><p>左：NSA尽管稀疏，但在<strong>一般基准</strong>、<strong>长期上下文任务</strong>和<strong>推理</strong>评估方面平均超过了Full Attention基线。</p><p>右：对于 64k 长度的序列处理，与 Full Attention 相比，NSA 在所有阶段（解码、前向传播和后向传播）都实现了显著的计算速度。</p><h2 id="nsa架构概述" tabindex="-1"><a class="header-anchor" href="#nsa架构概述"><span>NSA架构概述</span></a></h2><p><img src="'+n+'" alt="NSA架构概述"></p><p>论文中NSA架构概述。</p><p>左：框架通过三个并行的注意力分支处理输入序列：对于给定的查询，前面的键和值被处理为粗粒度模式的压缩注意力、重要标记块的选定注意力和本地上下文的滑动注意力。</p><p>右：每个分支产生的不同注意力模式的可视化。绿色区域表示需要计算注意力分数的区域，而白色区域表示可以跳过的区域。</p><h2 id="现有的稀疏注意力问题" tabindex="-1"><a class="header-anchor" href="#现有的稀疏注意力问题"><span>现有的稀疏注意力问题</span></a></h2><p>许多现有的稀疏注意力方法专注于减少 KV 缓存或理论计算减少，但难以在高级框架或后端中实现显著的延迟减少。NSA将高级架构和硬件高效实现相结合的算法，以充分利用稀疏性来提高模型效率。</p><p>现有的稀疏注意力方法主要针对推理，而训练中的计算挑战在很大程度上没有得到解决。这种限制阻碍了通过高效训练开发功能更强大的长上下文模型。</p><p>现有的稀疏注意力用于训练也有如下挑战：</p><ul><li>不可训练的组件。ClusterKV（包括 k-means 聚类）和 MagicPIG（包括基于 SimHash 的选择）等方法中的离散运算在计算图中造成了不连续性。这些不可训练的组件阻止了梯度流通过标记选择过程，从而限制了模型学习最佳稀疏模式的能力。</li><li>低效的反向传播。一些理论上可训练的稀疏注意力方法存在实践训练效率低下的问题。HashAttention 等方法中使用的令牌粒度选择策略，导致需要在注意力计算期间从 KV 缓存加载大量单独的令牌。这种非连续的内存访问阻止了对 FlashAttention 等快速注意技术的有效适应，这些技术依赖于连续的内存访问和分块计算来实现高吞吐量。因此，实施被迫回退到硬件利用率低，从而显著降低训练效率。</li></ul><h2 id="nsa-的算法设计和运算符实现" tabindex="-1"><a class="header-anchor" href="#nsa-的算法设计和运算符实现"><span>NSA 的算法设计和运算符实现</span></a></h2><p>主要是如何实现<strong>NSA架构概述</strong>图中的三个并行注意力，token如何压缩，token如何选择，滑动窗口等，具体细节可以查看论文：<a href="https://arxiv.org/abs/2502.11089" target="_blank" rel="noopener noreferrer">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a></p><h3 id="nsa-的内核设计" tabindex="-1"><a class="header-anchor" href="#nsa-的内核设计"><span>NSA 的内核设计</span></a></h3><p>为了在训练和预填充期间实现 FlashAttention 级别的加速，在 Triton 上实现了硬件对齐的稀疏注意力内核。鉴于 MHA 是内存密集型的，并且解码效率低下，于是专注于具有共享 KV 缓存的架构，如 GQA 和 MQA，遵循当前最先进的 LLMs。虽然压缩和滑动窗口注意力计算很容易与现有的 FlashAttention-2 内核兼容，但还是引入了用于稀疏选择注意力的专用内核设计。如果我们遵循 FlashNotice 的策略，将时间连续的查询块加载到 SRAM 中，这将导致内存访问效率低下，因为块内的查询可能需要不相交的 KV 块。为了解决这个问题，主要优化在于不同的查询分组策略：<strong>对于查询序列上的每个位置，我们将 GQA 组中的所有查询头（它们共享相同的稀疏 KV 块）加载到 SRAM 中</strong>。</p><p>NSA内核架构具有以下主要特性：</p><ol><li>以组为中心的数据加载。对于每个内部循环，在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>o</mi><msub><mi>n</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">position_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord mathnormal">i</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 处加载组中的所有 heads 查询 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mo stretchy="false">[</mo><mi>h</mi><mo separator="true">,</mo><mi>d</mi><mi>k</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">Q \\in \\mathbb{R} [h, dk]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">R</span><span class="mopen">[</span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">]</span></span></span></span> 及其共享的稀疏 key/value 块索引 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">I</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\\mathcal{I} t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal" style="margin-right:0.07382em;">I</span><span class="mord mathnormal">t</span></span></span></span>。</li><li>共享 KV 获取。在内部循环中，按顺序加载索引的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">I</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\\mathcal{I} t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal" style="margin-right:0.07382em;">I</span><span class="mord mathnormal">t</span></span></span></span> 连续键/值块到 SRAM 中，以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mo stretchy="false">[</mo><msub><mi>B</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>d</mi><mi>k</mi></msub><mo stretchy="false">]</mo><mo separator="true">,</mo><mspace width="1em"></mspace><mi>V</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mo stretchy="false">[</mo><msub><mi>B</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>d</mi><mi>v</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">K \\in \\mathbb{R} [B_k, d_k], \\quad V \\in \\mathbb{R} [B_k, d_v]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">R</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">R</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span> 最小化内存加载，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">B</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">\\mathcal{B} k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathcal" style="margin-right:0.03041em;">B</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> 内核块大小满足 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mi>k</mi></msub><mo>∣</mo><msup><mi>l</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">B_k \\mid l&#39;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7519em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> 。</li><li>网格上的外环。由于不同查询块的内部循环长度（与所选块数 n 成正比）几乎相同，因此我们将查询/输出循环放在 Triton 的网格调度器中，以简化和优化内核。</li></ol><p>这种设计通过以下方式实现近乎最佳的算术强度：</p><ol><li>通过分组共享消除冗余 KV 传输</li><li>在 GPU 流式多处理器之间平衡计算工作负载。</li></ol><p><img src="'+t+'" alt="NSA 的内核设计"></p><p>内核通过 GQA 组加载查询（网格循环），获取相应的稀疏 KV 块（Inner Loop），并在 SRAM 上执行注意力计算。绿色块表示 SRAM 上的数据，而蓝色块表示 HBM 上的数据。</p><h2 id="性能" tabindex="-1"><a class="header-anchor" href="#性能"><span>性能</span></a></h2><h3 id="longbench-基础测试" tabindex="-1"><a class="header-anchor" href="#longbench-基础测试"><span>LongBench 基础测试</span></a></h3><p><img src="'+p+'" alt="LongBench 基础测试"></p><h3 id="llmtest-needleinahaystack-检索能力测试" tabindex="-1"><a class="header-anchor" href="#llmtest-needleinahaystack-检索能力测试"><span>LLMTest_NeedleInAHaystack 检索能力测试</span></a></h3><p>使用<a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" target="_blank" rel="noopener noreferrer">LLMTest_NeedleInAHaystack</a>进行大海捞针检索能力测试</p><p><img src="'+e+'" alt="LLMTest_NeedleInAHaystack 检索能力测试"></p><p>在 64k 上下文长度下，跨上下文位置的大海捞针检索准确性。NSA 通过其分层稀疏注意力设计实现了完美的准确性。</p><h3 id="链式推理评估" tabindex="-1"><a class="header-anchor" href="#链式推理评估"><span>链式推理评估</span></a></h3><p><img src="'+m+'" alt="链式推理评估"></p><p>监督微调后的模型基于美国数学邀请考试 （AIME 24）的评估。NSA-R 在 8k 和 16k 序列长度上都表现出比 Full Attention-R 更好的性能。</p><h2 id="效率" tabindex="-1"><a class="header-anchor" href="#效率"><span>效率</span></a></h2><p>在 8-GPU A100 系统上评估了 NSA 与 Full Attention 的计算效率，如下：</p><p><img src="'+l+'" alt="NSA vs FlashAttention"></p><p>基于 Triton 的 NSA 内核与基于 Triton 的 FlashAttention-2 内核的比较。NSA显著减少了所有上下文长度的延迟，随着输入长度的增加，改进变得更加明显。</p><p>NSA vs FullAttention 训练速度对比：</p><p><img src="'+i+'" alt="NSA vs FullAttention"></p><p>相同上下文长度，NSA 的训练速度比 Full Attention 快，预期的加速比与内存访问量大致呈线性关系。</p><p>由于 Attention 的解码速度主要由内存访问瓶颈决定，而内存访问瓶颈与 KV 缓存的加载量密切相关。如上图所示，随着解码长度的增加，我们的方法表现出延迟显着减少，最高可达 11.6 × 64K 上下文长度的加速。内存访问效率的这种优势也会随着序列的延长而放大。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>NSA 是一种硬件对齐的稀疏注意力架构，用于高效的长期上下文建模。通过在可训练架构中将分层令牌压缩与块式令牌选择集成，NSA 架构实现了加速训练和推理，同时保持了 Full Attention 性能。NSA 通过展示一般基准性能与 Full Attention 基线相匹配，超越长期上下文评估中的建模能力，以及增强的推理能力，所有这些都伴随着计算延迟的可衡量减少并实现显著的加速，从而推动了最先进的技术。</p><p>NSA 向我们展示了，在不牺牲性能的情况下，通过优化算法，可以用更低的成本以更高的效率训练模型。</p><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考</span></a></h2><p><a href="https://arxiv.org/abs/2502.11089" target="_blank" rel="noopener noreferrer">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a></p>',53)]))}const k=r(g,[["render",d],["__file","kkpyfg4k.html.vue"]]),A=JSON.parse('{"path":"/article/kkpyfg4k.html","title":"Native Sparse Attention","lang":"zh-CN","frontmatter":{"title":"Native Sparse Attention","createTime":"2025/02/19 17:20:46","permalink":"/article/kkpyfg4k.html","watermark":true},"headers":[],"readingTime":{"minutes":7.2,"words":2161},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"llm/NativeSparseAttention.md","categoryList":[{"id":"dae1be","sort":10039,"name":"llm"}],"bulletin":false}');export{k as comp,A as data};
