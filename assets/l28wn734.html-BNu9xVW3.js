import{_ as a,c as r,a as n,o as t}from"./app-CS9K37Kg.js";const o={};function h(p,e){return t(),r("div",null,e[0]||(e[0]=[n('<p>LLM（大语言模型，Large Language Model）的测试基准（Benchmark）主要用于评估模型的性能，包括理解能力、推理能力、生成质量等。</p><p>以下是一些主流的 LLM 测试基准：</p><h2 id="通用能力评测" tabindex="-1"><a class="header-anchor" href="#通用能力评测"><span>通用能力评测</span></a></h2><p>(1) MMLU（Massive Multitask Language Understanding） • 涵盖57个学科，包括数学、历史、医学等，用于测试多任务学习能力。 • 代表论文：MMLU: Measuring Massive Multitask Language Understanding</p><p>(2) HELM（Holistic Evaluation of Language Models） • 斯坦福大学开发的全面测试框架，评估模型的公平性、透明度、准确性等多个维度。 • 官网：crfm.stanford.edu/helm/latest/</p><p>(3) BIG-bench（Beyond the Imitation Game Benchmark） • Google DeepMind 开发的大规模测试基准，包含204个任务，测试逻辑推理、编码能力等。 • 代表论文：BIG-bench: Evaluating the Future of Language Models</p><p>(4) Open LLM Leaderboard • Hugging Face 维护的 LLM 排行榜，基于不同测试基准进行评估。 • 官网：huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</p><h2 id="代码能力评测" tabindex="-1"><a class="header-anchor" href="#代码能力评测"><span>代码能力评测</span></a></h2><p>(5) HumanEval • OpenAI 提出的 Python 编程能力测试集，包含164个编程任务。 • 代表论文：Evaluating Large Language Models Trained on Code</p><p>(6) MBPP（Mostly Basic Python Programming） • 1000个Python编程任务，分为基础、中等、高级难度。 • 代表论文：Program Synthesis with Large Language Models</p><h2 id="逻辑推理与数学评测" tabindex="-1"><a class="header-anchor" href="#逻辑推理与数学评测"><span>逻辑推理与数学评测</span></a></h2><p>(7) GSM8K（Grade School Math 8K） • 小学数学问题测试集，考察数学推理能力。 • 代表论文：Training Verifiers to Solve Math Word Problems</p><p>(8) MATH • 更高阶的数学测试集，包括代数、几何、微积分等。 • 代表论文：Measuring Mathematical Problem Solving With the MATH Dataset</p><h2 id="事实知识与信息检索评测" tabindex="-1"><a class="header-anchor" href="#事实知识与信息检索评测"><span>事实知识与信息检索评测</span></a></h2><p>(9) TriviaQA • 开放域问答测试集，包含维基百科和网络数据。 • 代表论文：TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</p><p>(10) NaturalQuestions（NQ） • 由Google开发，测试模型回答真实世界问题的能力。 • 代表论文：Natural Questions: A Benchmark for Question Answering Research</p><h3 id="上下文检索能力测试" tabindex="-1"><a class="header-anchor" href="#上下文检索能力测试"><span>上下文检索能力测试</span></a></h3><h4 id="llmtest-needleinahaystack" tabindex="-1"><a class="header-anchor" href="#llmtest-needleinahaystack"><span>LLMTest_NeedleInAHaystack</span></a></h4><p>一个简单的 “大海捞针” 分析，用于测试长上下文LLMs的上下文检索能力。</p><p><a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" target="_blank" rel="noopener noreferrer">GitHub LLMTest_NeedleInAHaystack</a></p><h2 id="语言理解与对话评测" tabindex="-1"><a class="header-anchor" href="#语言理解与对话评测"><span>语言理解与对话评测</span></a></h2><p>(11) SuperGLUE • 经典NLP基准，包含问答、文本蕴涵、常识推理等任务。 • 代表论文：SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</p><p>(12) MT-Bench • LMSYS 开发，专门用于评测 LLM 的对话能力。 • 官网：lmsys.org</p><h2 id="多模态-文本-图像-评测" tabindex="-1"><a class="header-anchor" href="#多模态-文本-图像-评测"><span>多模态（文本+图像）评测</span></a></h2><p>(13) MMBench • 测试大模型的跨模态理解能力。 • 代表论文：MMBench: Is Your Multi-modal Model an Expert?</p><h2 id="测试基准" tabindex="-1"><a class="header-anchor" href="#测试基准"><span>测试基准</span></a></h2><h3 id="代码能力" tabindex="-1"><a class="header-anchor" href="#代码能力"><span>代码能力</span></a></h3><h4 id="swe-lancer" tabindex="-1"><a class="header-anchor" href="#swe-lancer"><span>SWE-Lancer</span></a></h4><p>今天（2025-02-18）凌晨2点，OpenAI开源了一个全新评估大模型<strong>代码能力</strong>的测试基准——<code>SWE-Lancer</code>。</p><p>目前，测试模型代码能力的基准主要有SWE-Bench和SWE-BenchVerified，但这两个有一个很大的局限性，主要针对孤立任务，很难反映现实中软件工程师的复杂情况。例如，开发人员需处理全技术栈的工作，要考虑代码库间的复杂交互和权衡。</p><p><a href="https://github.com/openai/SWELancer-Benchmark" target="_blank" rel="noopener noreferrer">Github SWELancer-Benchmark</a></p><p><a href="https://openai.com/index/swe-lancer/" target="_blank" rel="noopener noreferrer">OpenAI SWE-Lancer</a></p><h3 id="swe-bench-verified" tabindex="-1"><a class="header-anchor" href="#swe-bench-verified"><span>SWE-Bench Verified</span></a></h3><p>‌SWE-Bench Verified‌是由OpenAI推出的一种改进的基准测试工具，旨在更准确地评估AI模型在软件工程任务中的表现。SWE-BenchVerified是对原有SWE-Bench的改进版本，解决了原始版本中的一些问题，如单元测试过于严格、问题描述不明确和开发环境难以设置等‌</p><p><a href="https://openai.com/index/introducing-swe-bench-verified/" target="_blank" rel="noopener noreferrer">OpenAI SWE-Bench Verified</a></p><h3 id="swe-bench" tabindex="-1"><a class="header-anchor" href="#swe-bench"><span>SWE-Bench</span></a></h3><p>SWE-Bench 是一个用于评估大模型代码生成能力的基准测试集，它包含了 100 个代码生成任务，每个任务都来自于真实的软件开发场景。这些任务包括从简单的代码补全到复杂的代码重构和优化等，涵盖了软件开发中的各种常见问题。</p><p><a href="https://github.com/swe-bench/SWE-bench?tab=readme-ov-file" target="_blank" rel="noopener noreferrer">Github SWE-bench</a></p><h3 id="humaneval" tabindex="-1"><a class="header-anchor" href="#humaneval"><span>HumanEval</span></a></h3><p>HumanEval是一个用于评估大型语言模型 (LLM) 在代码生成任务中的参考基准，因为它使得对紧凑的函数级代码片段的评估变得容易。然而，关于其在评估 LLM 编程能力方面的有效性越来越多的担忧，主要问题是 HumanEval 中的任务太简单，可能不能代表真实世界的编程任务。相比于 HumanEval 中的算法导向任务，真实世界的软件开发通常涉及多样的库和函数调用。此外，LLM 在 HumanEval 上的表现还受<a href="https://arxiv.org/abs/2403.07974" target="_blank" rel="noopener noreferrer">污染和过拟合问题</a>的影响，这使得其在评估 LLM 的泛化能力方面不够可靠。</p><p><a href="https://github.com/openai/human-eval" target="_blank" rel="noopener noreferrer">OpenAI HumanEval</a></p><p><a href="https://arxiv.org/abs/2403.07974" target="_blank" rel="noopener noreferrer">污染和过拟合问题</a></p><h3 id="bigcodebench" tabindex="-1"><a class="header-anchor" href="#bigcodebench"><span>BigCodeBench</span></a></h3><p>BigCodeBench 可以在没有污染的情况下评估 LLM 解决实际和具有挑战性的编程任务的能力。具体来说，BigCodeBench 包含 1140 个函数级任务，挑战 LLM 遵循指令并将来自 139 个库的多个函数调用作为工具进行组合。为了严格评估 LLM，每个编程任务包含 5.6 个测试用例，平均分支覆盖率为 99%。</p><p>将 BigCodeBench 中的任务与代表性基准的任务进行了比较，包括 <a href="https://github.com/hendrycks/apps" target="_blank" rel="noopener noreferrer">APPS</a>、<a href="https://github.com/HKUNLP/DS-1000" target="_blank" rel="noopener noreferrer">DS-1000</a>、ODEX、<a href="https://github.com/ShishirPatil/gorilla/tree/main/data/apibench" target="_blank" rel="noopener noreferrer">APIBench</a>、<a href="https://github.com/google-research/google-research/tree/master/mbpp" target="_blank" rel="noopener noreferrer">MBPP</a>、<a href="https://github.com/microsoft/PyCodeGPT/tree/main/cert/pandas-numpy-eval" target="_blank" rel="noopener noreferrer">NumpyEval</a>、<a href="https://github.com/microsoft/PyCodeGPT/tree/main/cert/pandas-numpy-eval" target="_blank" rel="noopener noreferrer">PandasEval</a>、<a href="https://github.com/openai/human-eval" target="_blank" rel="noopener noreferrer">OpenAI HumanEval</a>和<a href="https://github.com/microsoft/PyCodeGPT/tree/main/apicoder/private-eval" target="_blank" rel="noopener noreferrer">TorchDataEval</a>。我们发现 BigCodeBench 需要更复杂的推理和问题解决技能来实现全面的功能。</p><p><a href="https://bigcode-bench.github.io/" target="_blank" rel="noopener noreferrer">GitHub Pages</a></p><p><a href="https://hf.co/spaces/bigcode/bigcodebench-leaderboard" target="_blank" rel="noopener noreferrer">Hugging Face Space</a></p><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考</span></a></h2><p><a href="https://github.com/openai/SWELancer-Benchmark" target="_blank" rel="noopener noreferrer">Github SWELancer-Benchmark</a></p><p><a href="https://openai.com/index/swe-lancer/" target="_blank" rel="noopener noreferrer">OpenAI SWE-Lancer</a></p><p><a href="https://openai.com/index/introducing-swe-bench-verified/" target="_blank" rel="noopener noreferrer">OpenAI SWE-Bench Verified</a></p><p><a href="https://github.com/swe-bench/SWE-bench?tab=readme-ov-file" target="_blank" rel="noopener noreferrer">Github SWE-bench</a></p><p><a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" target="_blank" rel="noopener noreferrer">GitHub LLMTest_NeedleInAHaystack</a></p><p><a href="https://github.com/openai/human-eval" target="_blank" rel="noopener noreferrer">OpenAI HumanEval</a></p><p><a href="https://arxiv.org/abs/2403.07974" target="_blank" rel="noopener noreferrer">污染和过拟合问题</a></p><p><a href="https://bigcode-bench.github.io/" target="_blank" rel="noopener noreferrer">GitHub Pages</a></p><p><a href="https://hf.co/spaces/bigcode/bigcodebench-leaderboard" target="_blank" rel="noopener noreferrer">Hugging Face Space</a></p><p><a href="https://github.com/hendrycks/apps" target="_blank" rel="noopener noreferrer">APPS</a></p><p><a href="https://github.com/HKUNLP/DS-1000" target="_blank" rel="noopener noreferrer">DS-1000</a></p><p><a href="https://github.com/ShishirPatil/gorilla/tree/main/data/apibench" target="_blank" rel="noopener noreferrer">APIBench</a></p><p><a href="https://github.com/microsoft/PyCodeGPT/tree/main/cert/pandas-numpy-eval" target="_blank" rel="noopener noreferrer">NumpyEval</a></p><p><a href="https://github.com/microsoft/PyCodeGPT/tree/main/cert/pandas-numpy-eval" target="_blank" rel="noopener noreferrer">PandasEval</a></p><p><a href="https://github.com/microsoft/PyCodeGPT/tree/main/apicoder/private-eval" target="_blank" rel="noopener noreferrer">TorchDataEval</a></p><p><a href="https://github.com/google-research/google-research/tree/master/mbpp" target="_blank" rel="noopener noreferrer">MBPP</a></p>',64)]))}const i=a(o,[["render",h],["__file","l28wn734.html.vue"]]),l=JSON.parse('{"path":"/llm/l28wn734.html","title":"LLM 测试基准","lang":"zh-CN","frontmatter":{"title":"LLM 测试基准","createTime":"2025/02/19 11:07:44","permalink":"/llm/l28wn734.html","watermark":true},"headers":[],"readingTime":{"minutes":4.98,"words":1494},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/LLM测试基准.md","bulletin":false}');export{i as comp,l as data};
