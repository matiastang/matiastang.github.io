import{_ as a,c as n,a as r,o as t}from"./app-CS9K37Kg.js";const o={};function i(s,e){return t(),n("div",null,e[0]||(e[0]=[r('<h2 id="deepseek-ai" tabindex="-1"><a class="header-anchor" href="#deepseek-ai"><span>DeepSeek AI</span></a></h2><h3 id="《native-sparse-attention》-2025" tabindex="-1"><a class="header-anchor" href="#《native-sparse-attention》-2025"><span>《Native Sparse Attention》(2025)</span></a></h3><p>2025年2月16日，DeepSeek 公布了论文 <a href="https://arxiv.org/abs/2502.11089" target="_blank" rel="noopener noreferrer">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a>。</p><p>NSA（Native Sparse Attention）是一种原生可训练的稀疏注意力机制，它运用创新算法与实现硬件对齐，以实现高效的长上下文建模。</p><p><strong>注意</strong> NSA是可训练的注意力机制</p><h2 id="nsa-原理" tabindex="-1"><a class="header-anchor" href="#nsa-原理"><span>NSA 原理</span></a></h2><p>NSA 采用动态分层稀疏策略，将粗粒度令牌压缩与细粒度令牌选择相结合，以保持全局上下文感知和局部精度。通过下面两项关键创新推进了稀疏注意力设计：</p><ol><li>通过算术强度平衡算法设计实现了大幅加速，并针对现代硬件进行了优化。</li><li>支持端到端训练，在不牺牲模型性能的情况下减少训练计算量。</li></ol><h3 id="flashmla" tabindex="-1"><a class="header-anchor" href="#flashmla"><span>FlashMLA</span></a></h3><p><a href="https://github.com/deepseek-ai/FlashMLA" target="_blank" rel="noopener noreferrer">GitHub FlashMLA</a></p><p>2025年2月24 DeepSeek开源了FlashMLA。FlashMLA 是适用于 Hopper GPU 的高效 MLA 解码内核，针对可变长度序列服务进行了优化。</p><p>当前发布：</p><ul><li>BF16</li><li>Paged kvcache with block size of 64</li></ul><p>使用 CUDA 12.6，在 H800 SXM5 上实现高达 3000 GB/s 的内存绑定配置和 580 TFLOPS 的计算绑定配置。</p><h2 id="moonshot-ai" tabindex="-1"><a class="header-anchor" href="#moonshot-ai"><span>MoonShot AI</span></a></h2><h3 id="《moba-mixture-of-block-attention-for-long-context-llms》-2025" tabindex="-1"><a class="header-anchor" href="#《moba-mixture-of-block-attention-for-long-context-llms》-2025"><span>《MoBA Mixture of Block Attention for Long-Context LLMs》（2025）</span></a></h3><p>2025年2月18日，MoonShot 公布了论文 <a href="https://arxiv.org/abs/2502.13189" target="_blank" rel="noopener noreferrer">MoBA: Mixture of Block Attention for Long-Context LLMs</a>。</p><p>MoBA (Mixture of Block Attention)，这是一种将专家混合 （MoE） 原则应用于注意力机制的创新方法。这种新颖的架构在长上下文任务上表现出卓越的性能，同时提供了一个关键优势：<strong>能够在全注意力和稀疏注意力之间无缝转换，从而提高效率，而不会影响性能</strong>。</p><h2 id="transformer到chatgpt里程碑式论文" tabindex="-1"><a class="header-anchor" href="#transformer到chatgpt里程碑式论文"><span>Transformer到ChatGPT里程碑式论文</span></a></h2><p>###《Neural Machine Translation by Jointly Learning to Align and Translate》(2014)</p><p>这篇论文为循环神经网络（RNN）引入了注意力机制，以提升长距离序列建模的能力，使得RNN可以更准确地翻译更长的句子，也是原始Transformer模型的开发动机。</p><p>论文链接：<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener noreferrer">Neural Machine Translation by Jointly Learning to Align and Translate</a></p><h3 id="《attention-is-all-you-need》-2017" tabindex="-1"><a class="header-anchor" href="#《attention-is-all-you-need》-2017"><span>《Attention is All You Need》(2017)</span></a></h3><p>这篇论文提出了Transformer模型，该模型完全基于自注意力机制，不再使用RNN或CNN，从而实现了更高效的序列建模。Transformer模型在机器翻译、文本生成等任务上取得了显著的性能提升，为后续的BERT、GPT等模型奠定了基础。</p><p>论文链接：<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener noreferrer">《Attention is All You Need》</a></p><h3 id="《on-layer-normalization-in-the-transformer-architecture》-2020" tabindex="-1"><a class="header-anchor" href="#《on-layer-normalization-in-the-transformer-architecture》-2020"><span>《On Layer Normalization in the Transformer Architecture》(2020)</span></a></h3><p>这篇论文研究了Transformer模型中的层归一化（Layer Normalization）对模型性能的影响，并提出了改进的层归一化方法，使得Transformer模型在长文本处理任务上取得了更好的性能。</p><p>论文链接：<a href="https://arxiv.org/abs/2002.04745" target="_blank" rel="noopener noreferrer">《On Layer Normalization in the Transformer Architecture》</a></p><p>Transformer架构论文中的层归一化表明Pre-LN也很有效，解决了梯度问题，许多模型也在实践中采用Pre-LN，缺点在于可能会导致表示秩崩溃。</p><p>虽然业界关于使用Post-LN还是Pre-LN仍然存在争论，但最近有一篇新论文提出同时利用这两种方法，不过在实践中是否有用仍然需要进一步观察。</p><p>论文链接：<a href="https://arxiv.org/abs/2304.14802" target="_blank" rel="noopener noreferrer">《Transformer with Dual Residual Connections》</a></p><h3 id="《learning-to-control-fast-weight-memories-an-alternative-to-dynamic-recurrent-neural-networks》-1991" tabindex="-1"><a class="header-anchor" href="#《learning-to-control-fast-weight-memories-an-alternative-to-dynamic-recurrent-neural-networks》-1991"><span>《Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Neural Networks》(1991)</span></a></h3><p>在1991年，也就是原始Transformer论文发布之前大约25年，Juergen Schmidhuber提出了一种替代循环神经网络的方法，叫做快速权重编程器（FWP, Fast Weight Programmers）</p><p>论文链接：<a href="https://ieeexplore.ieee.org/document/6796337" target="_blank" rel="noopener noreferrer">《Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Neural Networks》</a></p><p>FWP方法用到了一个前馈神经网络，通过梯度下降来缓慢学习以对另一神经网络的快速权重变化进行编程。</p><p>而发展到今天的Transformer术语中，FROM和TO分别被称为键（key）和值（value），应用快速网络的INPUT叫做查询（query）。</p><p>从本质上讲，查询是由快速权重矩阵处理的，是键和值的外积之和（不考虑归一化和投影的话）。</p><p>由于两个网络的所有操作都是可微的，通过加性外积或二阶张量积获得快速权重变化的端到端可微主动控制。</p><p>因此，慢速网络可以通过梯度下降来学习，以在序列处理期间快速修改快速网络，在数学上等同于不包括归一化的，后来也叫做具有线性化自注意力的Transformer，即线性Transformer</p><p>2021年，一篇论文明确证明了线性化自注意力与20世纪90年代的快速权重编程器之间的等价性。</p><p>论文链接：<a href="https://arxiv.org/pdf/2102.11174.pdf" target="_blank" rel="noopener noreferrer">《Linear Transformers Are Secretly Fast Weight Programmers》</a></p><h3 id="《universal-language-model-fine-tuning-for-text-classification》-2018" tabindex="-1"><a class="header-anchor" href="#《universal-language-model-fine-tuning-for-text-classification》-2018"><span>《Universal Language Model Fine-tuning for Text Classification》 (2018)</span></a></h3><p>这篇论文虽然发表于2018年，但并没有研究Transformer，而主要关注循环神经网络，但提出了有效的预训练语言模型和对下游任务的迁移学习。</p><p>论文链接：<a href="https://arxiv.org/abs/1801.06146" target="_blank" rel="noopener noreferrer">《Universal Language Model Fine-tuning for Text Classification》</a></p><p>虽然迁移学习最早是在计算机视觉中提出的，但当时在自然语言处理（NLP）领域中还没有普及。</p><p>ULMFit是最早证明预训练语言模型并在特定任务上对其进行微调可以在许多NLP任务中实现最先进性能的论文之一。</p><p>ULMFit提出的微调语言模型的三阶段过程如下：</p><ol><li>在大型文本语料库上训练语言模型</li><li>在特定任务的数据上微调预训练的语言模型，使其适应文本的特定风格和词汇</li><li>通过逐层解冻来微调特定任务数据的分类器，以避免灾难性遗忘</li></ol><p>该方法，即在大型语料库上训练语言模型，然后在下游任务上对其进行微调，是基于Transformer的基础模型（如BERT、GPT-2/3/4、RoBERTa等）中使用的核心方法。</p><p>不过ULMFiT的关键组件是逐层解冻，通常无法在Transformer架构中实现，其中所有层通常只经过一次微调。</p><h3 id="《bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding》-2018" tabindex="-1"><a class="header-anchor" href="#《bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding》-2018"><span>《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》 (2018)</span></a></h3><p>在Transformer架构提出之后，大型语言模型研究开始分为两个方向：用于预测建模任务（如文本分类）的编码器Transformer；以及用于生成建模任务（如翻译、摘要和其他形式的文本创建）的解码器Transformer</p><p>论文链接：<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</a></p><p>BERT论文提出了遮罩语言建模的概念，并且下一句预测（next-sentence prediction）仍然是一种有影响力的解码器架构，不过后续的RoberTa删除了下一句预测任务，简化了预训练目标。</p><h3 id="《improving-language-understanding-by-generative-pre-training》-2018" tabindex="-1"><a class="header-anchor" href="#《improving-language-understanding-by-generative-pre-training》-2018"><span>《Improving Language Understanding by Generative Pre-Training》 (2018)</span></a></h3><p>第一版GPT论文提出了解码器架构，以及使用下一个单词预测进行预训练。</p><p>论文链接：<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener noreferrer">《Improving Language Understanding by Generative Pre-Training》</a></p><p>BERT使用的遮罩语言模型预训练目标，所以是双向Transformer模型；而GPT是单向自回归模型，但其学到的嵌入也可以用于分类。</p><p>GPT方法是当下最有影响力的大型语言模型（如chatGPT）的核心技术。</p><p>后续发布的GPT-2和GPT-3论文说明了LLM能够进行零样本和少样本学习，指出了大型语言模型的涌现能力。</p><p>GPT-3仍然是训练当下语言模型（如ChatGPT）的常用基线和基础模型。</p><h3 id="《bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension》-2019" tabindex="-1"><a class="header-anchor" href="#《bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension》-2019"><span>《BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension》 (2019)</span></a></h3><p>如前所述，BERT类语言模型主要关注编码器，通常是预测建模任务的首选，而GPT类型的解码器风格的语言模型在文本生成方面更好。</p><p>论文链接：<a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener noreferrer">《BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension》</a></p><p>为了同时利用二者的优势，BART论文结合了编码器和解码器部分。</p><h3 id="《harnessing-the-power-of-llms-in-practice-a-survey-on-chatgpt-and-beyond》-2023" tabindex="-1"><a class="header-anchor" href="#《harnessing-the-power-of-llms-in-practice-a-survey-on-chatgpt-and-beyond》-2023"><span>《Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond》 (2023)</span></a></h3><p>除了讨论BERT风格的遮罩语言模型（编码器）和GPT风格的自回归语言模型（解码器）之外，还提供了关于预训练和微调数据的讨论和指导。</p><p>如果想了解更多关于提高Transformer效率的各种技术，还可以阅读两篇综述。</p><p>论文链接：<a href="https://arxiv.org/abs/2009.06732" target="_blank" rel="noopener noreferrer">Efficient Transformers: A Survey</a></p><p>论文链接：<a href="https://arxiv.orgpdf/2302.01107.pdf" target="_blank" rel="noopener noreferrer">A Survey of Transformer Models for Natural Language Processing</a></p><h3 id="《flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness》-2022" tabindex="-1"><a class="header-anchor" href="#《flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness》-2022"><span>《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》 (2022)</span></a></h3><p>论文链接：<a href="https://arxiv.org/pdf/2205.14135.pdf" target="_blank" rel="noopener noreferrer">《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》</a></p><p>虽然大多数transformer论文都没有替换原始的缩放点积机制来改进自注意力，但FlashAttention是其中最常引用的一种机制。</p><h3 id="《cramming-training-a-language-model-on-a-single-gpu-in-one-day》-2022" tabindex="-1"><a class="header-anchor" href="#《cramming-training-a-language-model-on-a-single-gpu-in-one-day》-2022"><span>《Cramming: Training a Language Model on a Single GPU in One Day》 (2022)</span></a></h3><p>在这篇论文中，研究人员使用单个GPU用了24个小时训练了一个遮罩语言模型/编码器风格的语言模型，在单个GPU上进行24小时，相比之下，2018年BERT刚提出来的时候，在16个TPU上训练了四天。</p><p>论文链接：<a href="https://arxiv.org/abs/2212.14034" target="_blank" rel="noopener noreferrer">《Cramming: Training a Language Model on a Single GPU in One Day》</a></p><p>一个有趣的结论是，虽然较小的模型具有更高的吞吐量，但小模型的学习效率也比较低，所以较大的模型不需要更多的训练时间来达到特定的预测性能阈值。</p><h3 id="《lora-low-rank-adaptation-of-large-language-models》-2021" tabindex="-1"><a class="header-anchor" href="#《lora-low-rank-adaptation-of-large-language-models》-2021"><span>《LoRA: Low-Rank Adaptation of Large Language Models》(2021)</span></a></h3><p>在大型数据集上预训练的现代大型语言模型展现出了涌现能力，并在各种任务上都实现了非常强大的性能，包括多语言翻译、摘要、编码和问答。</p><p>论文链接：<a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noopener noreferrer">《LoRA: Low-Rank Adaptation of Large Language Models》</a></p><p>不过如果想提高Transformer在特定领域数据和特定任务上的性能，那么就需要对Transformer进行微调。</p><p>低秩自适应（LoRA）是一种参数高效（parameter-efficient）的方式来微调大型语言模型，相比其他方法，LoRA既优雅又非常通用，可以应用于其他类型的模型。</p><p>虽然预训练模型的权重在预训练任务上具有满秩，但LoRA作者指出，预训练的大型语言模型在适应新任务时具有较低的「内在维度」。</p><p>因此，LoRA背后的主要思想是将权重变化ΔW分解为更低秩的表示，即更高效的参数。</p><h3 id="《scaling-down-to-scale-up-a-guide-to-parameter-efficient-fine-tuning》-2022" tabindex="-1"><a class="header-anchor" href="#《scaling-down-to-scale-up-a-guide-to-parameter-efficient-fine-tuning》-2022"><span>《Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning》 (2022)</span></a></h3><p>这篇综述回顾了40多篇关于参数高效微调方法，包括prefix调整、adapter和LoRA等。</p><p>论文链接：<a href="https://arxiv.org/abs/2303.15647" target="_blank" rel="noopener noreferrer">《Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning》</a></p><h3 id="《scaling-language-models-methods-analysis-insights-from-training-gopher》-2022" tabindex="-1"><a class="header-anchor" href="#《scaling-language-models-methods-analysis-insights-from-training-gopher》-2022"><span>《Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher》 (2022)</span></a></h3><p>论文链接：<a href="https://arxiv.org/abs/2112.11446" target="_blank" rel="noopener noreferrer">《Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher》</a></p><p>Gopher论文中有大量的分析来理解大型语言模型的训练过程。</p><p>研究人员在3000亿个token上训练了一个80层、2800亿参数的模型，还提出了一些架构上的修改，如使用RMSNorm（均方根归一化）而非LayerNorm（层归一化）。</p><p>LayerNorm和RMSNorm都优于BatchNorm，因为它们并不依赖于batch size，也不需要同步，对于在batch size较小的分布式设置中是一个优势，而且RMSNorm通常被认为可以稳定更深层次架构中的训练。</p><p>这篇论文的主要重点是不同尺度（sacle）模型在任务性能上的分析。</p><p>对152个不同任务的评估表明，增加模型尺寸对理解、事实核查和有毒语言识别等任务的益处最大，而与逻辑和数学推理相关的任务从架构扩展中受益较少。</p><h3 id="《training-compute-optimal-large-language-models》-2022" tabindex="-1"><a class="header-anchor" href="#《training-compute-optimal-large-language-models》-2022"><span>《Training Compute-Optimal Large Language Models》 (2022)</span></a></h3><p>这篇论文提出了700亿参数Chinchilla模型，在生成建模任务上优于常用的1750亿参数GPT-3模型，不过这篇文章的主要贡献是发现目前大型语言模型存在「严重训练不足」的问题。</p><p>论文链接：<a href="https://arxiv.org/abs/2203.15556" target="_blank" rel="noopener noreferrer">《Training Compute-Optimal Large Language Models》</a></p><p>论文中定义了大型语言模型训练的线性缩放律（linear scaling low），例如虽然Chinchilla的大小只有GPT-3的一半，但它的表现优于GPT-3，因为它是在1.4万亿（而不是3000亿）个token上训练的。</p><p>换句话说，训练语料中token的数量与模型大小一样重要。</p><h3 id="《pythia-a-suite-for-analyzing-large-language-models-across-training-and-scaling》-2023" tabindex="-1"><a class="header-anchor" href="#《pythia-a-suite-for-analyzing-large-language-models-across-training-and-scaling》-2023"><span>《Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling》 (2023)</span></a></h3><p>Pythia是一组开源的大型语言模型，参数量从7千万到120亿不等，以用于研究大型语言模型在训练过程中的演变。</p><p>论文链接：<a href="https://arxiv.org/abs/2304.01373" target="_blank" rel="noopener noreferrer">《Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling》</a></p><p>模型架构类似于GPT-3，但包括一些组件改进，例如用Flash Attention和Rotary Positional Embeddings。</p><p>Pythia在Pile数据集（825 Gb）上训练了3000亿个token，在regular PILE上训练约1个epoch，deduplicated PILE上训练约1.5个epoch</p><p>Pythia研究的主要结论如下：</p><ol><li>在重复数据上进行训练（超过1个epoch）不会提升或降低性能。</li><li>训练顺序不会影响记忆。这个结论让我们无法通过重新排序训练数据来缓解不希望的逐字记忆问题。</li><li>预训练词频影响任务性能。例如，对于更频繁的术语，少样本学习往往准确度更高。</li><li>将batch size加倍可以将训练时间减半，但不会影响收敛。</li></ol><p><strong>对齐</strong>：让大型语言模型符合预期目标</p><p>近年来，我们看到了许多相对强大的大型语言模型，可以生成类人的文本（例如GPT-3和Chinchilla等），但常用的预训练范式似乎已经达到了上限。</p><p>为了使语言模型对人类更有帮助并减少错误信息和有害语言，研究人员设计了额外的训练范式来微调预训练的基础模型。</p><h3 id="《training-language-models-to-follow-instructions-with-human-feedback》-2022" tabindex="-1"><a class="header-anchor" href="#《training-language-models-to-follow-instructions-with-human-feedback》-2022"><span>《Training Language Models to Follow Instructions with Human Feedback》 (2022)</span></a></h3><p>在这篇提出InstructGPT模型论文中，研究人员使用了一种强化学习机制，其中包括人类参与反馈的循环机制（RLHF）。</p><p>论文链接：<a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noopener noreferrer">《Training Language Models to Follow Instructions with Human Feedback》</a></p><p>研究人员从预训练的GPT-3基础模型开始，使用监督学习对人类生成的提示与模型回复进行进一步微调；然后要求人类对模型输出进行排名，以训练奖励模型；最后使用奖励模型通过近端策略优化（PPO, proximal policy optimization）使用强化学习来更新预训练和微调的GPT-3模型。</p><p>这篇论文也被称为描述ChatGPT背后想法的论文，也有传言说ChatGPT是InstructGPT的放大版本，在更大的数据集上进行了微调。</p><h3 id="《constitutional-ai-harmlessness-from-ai-feedback》-2022" tabindex="-1"><a class="header-anchor" href="#《constitutional-ai-harmlessness-from-ai-feedback》-2022"><span>《Constitutional AI: Harmlessness from AI Feedback》(2022)</span></a></h3><p>这篇论文中，研究人员将对齐思想更进一步，提出了一种创建无害AI系统的训练机制。</p><p>论文链接：<a href="https://arxiv.org/abs/2212.08073" target="_blank" rel="noopener noreferrer">《Constitutional AI: Harmlessness from AI Feedback》</a></p><p>文中提出了一种基于规则列表（由人类提供）的自训练机制，而非人类监督。</p><p>与上面提到的InstructGPT论文类似，这种机制也使用强化学习方法。</p><h3 id="《self-instruct-aligning-language-model-with-self-generated-instruction》-2022" tabindex="-1"><a class="header-anchor" href="#《self-instruct-aligning-language-model-with-self-generated-instruction》-2022"><span>《Self-Instruct: Aligning Language Model with Self Generated Instruction》 (2022)</span></a></h3><p>指令微调是从GPT-3之类的预训练基础模型发展到ChatGPT类更强大语言模型的关键技术。</p><p>论文链接：<a href="https://arxiv.org/abs/2212.10560" target="_blank" rel="noopener noreferrer">《Self-Instruct: Aligning Language Model with Self Generated Instruction》</a></p><p>开源的人工生成指令数据集，如databricks-dolly-15 k，可以帮助调优，但想要进一步扩大指令数据集的规模，可以从语言模型中自举得到。</p><p>Self-Instruct是一种几乎无需标注，即可将预训练的LLM与指令对齐的方法，总共包括4个步骤：</p><ol><li>用一组人工编写的指令和样本指令作为种子任务池。</li><li>使用预训练的语言模型（如GPT-3）来确定任务类别。</li><li>给定新指令，让预训练的语言模型生成回复。</li><li>在将回复添加到任务池之前，收集、修剪和筛选这些响应。</li></ol><p>在实践中，整个过程可以基于ROUGE来评分，可以认为Self-Instruct-finetuned LLM的性能优于GPT-3基础LLM，并且可以与在大型人类编写的指令集上预训练的LLM竞争，self-instruct也可以使已经根据人类指令进行微调的LLM受益。</p><p>当然，评估语言模型的黄金标准是询问人类评分员。</p><p>基于人类评估，Self-Instruct优于基本LLM和以监督方式在人类指令数据集上训练的LLM（SuperNI，T0 Trainer），但有趣的是，Self-Instruct并没有优于通过人工反馈强化学习（RLHF）训练的方法。</p><p><strong>强化学习与人类反馈（RLHF）</strong></p><p>虽然RLHF（基于人类反馈的强化学习）可能无法完全解决LLM当前的问题，但它目前被认为是可用的最佳选择，特别是与上一代LLM相比。</p><p>未来很可能会看到更多创造性的方法将RLHF应用于LLM其他领域。</p><p>上面提到的两篇论文InstructGPT和Consitutinal AI利用了RLHF，但从技术上来说，Consitutinal AI使用的是AI反馈而非人类反馈。</p><h3 id="《asynchronous-methods-for-deep-reinforcement-learning》-2016" tabindex="-1"><a class="header-anchor" href="#《asynchronous-methods-for-deep-reinforcement-learning》-2016"><span>《Asynchronous Methods for Deep Reinforcement Learning》 (2016)</span></a></h3><p>论文链接：<a href="https://arxiv.org/pdf/1602.01783.pdf" target="_blank" rel="noopener noreferrer">《Asynchronous Methods for Deep Reinforcement Learning》</a></p><p>这篇论文引入了策略梯度方法作为基于深度学习的RL中Q学习的替代方案。</p><h3 id="《proximal-policy-optimization-algorithms》-2017" tabindex="-1"><a class="header-anchor" href="#《proximal-policy-optimization-algorithms》-2017"><span>《Proximal Policy Optimization Algorithms》 (2017)</span></a></h3><p>论文链接：<a href="https://arxiv.org/abs/1909.08593" target="_blank" rel="noopener noreferrer">《Proximal Policy Optimization Algorithms》</a></p><p>这篇论文提出了一种改进的基于近似策略的强化学习过程，比上面的策略优化算法更具数据效率和可扩展性。</p><h3 id="《fine-tuning-language-models-from-human-preferences》-2020" tabindex="-1"><a class="header-anchor" href="#《fine-tuning-language-models-from-human-preferences》-2020"><span>《Fine-Tuning Language Models from Human Preferences》 (2020)</span></a></h3><p>论文链接：<a href="https://arxiv.org/abs/1909.08593" target="_blank" rel="noopener noreferrer">《Fine-Tuning Language Models from Human Preferences》</a></p><p>这篇论文说明了PPO的概念和对预训练语言模型的奖励学习，包括KL正则化，以防止策略与自然语言偏离太远。</p><h3 id="《learning-to-summarize-from-human-feedback》-2022" tabindex="-1"><a class="header-anchor" href="#《learning-to-summarize-from-human-feedback》-2022"><span>《Learning to Summarize from Human Feedback》 (2022)</span></a></h3><p>论文链接：<a href="https://arxiv.org/abs/2009.01325" target="_blank" rel="noopener noreferrer">《Learning to Summarize from Human Feedback》</a></p><p>这篇论文提出了常用的RLHF三步程序：</p><ol><li>预训练GPT-3</li><li>以有监督的方式进行微调</li><li>同样以有监督的方式训练奖励模型，然后使用具有邻近策略优化的奖励模型来训练微调模型。</li></ol><p>论文还表明，与常规有监督学习相比，具有近似策略优化的强化学习可以产生更好的模型。</p><h3 id="《training-language-models-to-follow-instructions-with-human-feedback》-2022-1" tabindex="-1"><a class="header-anchor" href="#《training-language-models-to-follow-instructions-with-human-feedback》-2022-1"><span>《Training Language Models to Follow Instructions with Human Feedback》 (2022)</span></a></h3><p>这篇论文提出InstructGPT使用与上述RLHF类似的三步过程，但不是总结文本，而是专注于基于人类指令生成文本。</p><p>论文链接：<a href="https://arxiv.org/pdf/2203.02155.pdf" target="_blank" rel="noopener noreferrer">《Training Language Models to Follow Instructions with Human Feedback》</a></p><p>除此之外，还使用一个标签器来从最好到最差对输出进行排名，而不仅仅是人类和AI生成的文本之间的二元比较。</p><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h3><p>读完上面列出的论文，就可以了解到当前大型语言模型背后的设计、约束和演变过程，下面是一些可用的资源。</p><p>GPT的开源平替：</p><p>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model (2022), https://arxiv.org/abs/2211.05100</p><p>OPT: Open Pre-trained Transformer Language Models (2022), https://arxiv.org/abs/2205.01068</p><p>UL2: Unifying Language Learning Paradigms (2022), https://arxiv.org/abs/2205.05131</p><p>ChatGPT的替代方案：</p><p>LaMDA: Language Models for Dialog Applications (2022), https://arxiv.org/abs/2201.08239</p><p>(Bloomz) Crosslingual Generalization through Multitask Finetuning (2022), https://arxiv.org/abs/2211.01786</p><p>(Sparrow) Improving Alignment of Dialogue Agents via Targeted Human Judgements (2022), https://arxiv.org/abs/2209.14375</p><p>BlenderBot 3: A Deployed Conversational Agent that Continually Learns to Responsibly Engage, https://arxiv.org/abs/2208.03188</p><p>计算生物学领域的大型语言模型</p><p>ProtTrans：Towards Cracking the Language of Life’s Code Through Self-Supervised Deep Learning and High Performance Computing（2021）, https://arxiv.org/abs/2007.06225</p><p>Highly Accurate Protein Structure Prediction with AlphaFold (2021), https://www.nature.com/articles/s41586-021-03819-2</p><p>Large Language Models Generate Functional Protein Sequences Across Diverse Families (2023), https://www.nature.com/articles/s41587-022-01618-2</p><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考</span></a></h2><p><a href="https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure" target="_blank" rel="noopener noreferrer">Ahead of AI</a></p><p><a href="https://baijiahao.baidu.com/s?id=1767302862707584748&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener noreferrer">20+篇里程碑式论文，带你从「Transformer的前世」速通到ChatGPT</a></p>',168)]))}const l=a(o,[["render",i],["__file","u8nq4l6c.html.vue"]]),g=JSON.parse('{"path":"/llm/u8nq4l6c.html","title":"里程碑式论文","lang":"zh-CN","frontmatter":{"title":"里程碑式论文","createTime":"2025/03/21 11:24:17","permalink":"/llm/u8nq4l6c.html","watermark":true},"headers":[],"readingTime":{"minutes":16.96,"words":5089},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/里程碑式论文.md","bulletin":false}');export{l as comp,g as data};
