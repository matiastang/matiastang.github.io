import{_ as n,c as o,b as e,o as l}from"./app-CS9K37Kg.js";const a={};function r(i,t){return l(),o("div",null,t[0]||(t[0]=[e("p",null,"注意力机制（Attention）允许对依赖关系进行建模，而不用考虑它们在输入或输出序列中的距离。",-1),e("p",null,"自注意力（Self-attention，有时称为内注意力 intra-attention）是一种将单一序列不同位置相关联的注意力机制，可以计算序列的表示形式。",-1),e("p",null,"多头注意力（Multi-Head Attention）",-1),e("p",null,"“减少顺序计算”这一目标也构成了扩展神经 GPU（Extended Neural GPU） [16]、ByteNet [18] 和 ConvS2S [9] 的基础，所有这些都使用卷积神经网络作为基本模块，并行地计算所有输入和输出位置的隐藏表示（hidden representations）。在这些模型中，将来自两个任意输入或输出位置的信号关联起来所需的操作数，随位置之间的距离而增加， ConvS2S 为线性增长， ByteNet 则是对数增长。这使得学习远距离的依赖关系变得更加困难 [12]。在 Transformer 中，这被减少至常数次操作，但这也导致平均注意力加权位置信息而使有效分辨率降低，我们用多头注意力（Multi-Head Attention）来抵消这种影响",-1),e("h2",{id:"参考",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#参考"},[e("span",null,"参考")])],-1),e("p",null,[e("a",{href:"https://arxiv.org/html/1706.03762v7",target:"_blank",rel:"noopener noreferrer"},"Attention Is All You Need 第7版本")],-1),e("p",null,[e("a",{href:"https://zhuanlan.zhihu.com/p/682007654",target:"_blank",rel:"noopener noreferrer"},"Attention is All you Need 全文翻译")],-1)]))}const m=n(a,[["render",r],["__file","0sqnokd9.html.vue"]]),d=JSON.parse('{"path":"/llm/google/0sqnokd9.html","title":"Attention Is All You Need","lang":"zh-CN","frontmatter":{"title":"Attention Is All You Need","createTime":"2025/02/19 17:44:07","permalink":"/llm/google/0sqnokd9.html","watermark":true},"headers":[],"readingTime":{"minutes":1.2,"words":359},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/google/Attention.md","bulletin":false}');export{m as comp,d as data};
