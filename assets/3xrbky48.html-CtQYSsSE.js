import{_ as t,c as r,b as a,o}from"./app-CS9K37Kg.js";const c={};function n(l,e){return o(),r("div",null,e[0]||(e[0]=[a("p",null,"Transformer 中的 KV Cache 是指在自注意力机制中，为了提高计算效率，将键（Key）和值（Value）存储在缓存中，以便在后续的注意力计算中重复使用。KV Cache可以显著减少计算量，提高模型性能。",-1),a("p",null,"对于一次推理过程，Decoder每输出一个token就要做一次运算，你会发现这个K和V都来自Encoder部分，其实就是输入序列算出来的，每一层的Decoder都有一份，那就没有必要每次都重算一遍了，直接把这批KV存起来就可以了，那每一层有多少个kv cache呢？你可以看到其实就是d_model个，也就是所谓的hidden_size，理解KV cache的显存计算就再也没有问题了。",-1)]))}const s=t(c,[["render",n],["__file","3xrbky48.html.vue"]]),m=JSON.parse('{"path":"/ai/3xrbky48.html","title":"KV Cache","lang":"zh-CN","frontmatter":{"title":"KV Cache","createTime":"2025/03/25 15:17:49","permalink":"/ai/3xrbky48.html","watermark":true},"headers":[],"readingTime":{"minutes":0.77,"words":232},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/ai/KVCache.md","bulletin":false}');export{s as comp,m as data};
