import{_ as r,c as a,a as o,o as n}from"./app-CS9K37Kg.js";const t={};function s(p,e){return n(),a("div",null,e[0]||(e[0]=[o(`<p>TensorFlow 是数据科学家、软件开发者和教育工作者主要使用的开源平台，用于使用数据流图形进行机器学习。图像中的节点代表数学运算，而图像边缘则代表节点间流动的多维数据阵列（张量）。这种灵活的架构允许将机器学习算法描述为相关运算的图形。可以在便携式设备、台式电脑和高端服务器等众多不同平台的 GPU、CPU 和 TPU 上训练和执行这些代码，而无需重写代码。这意味着各种背景的编程人员均可以使用相同的工具集进行协作，从而显著提高效率。该系统最初由 Google Brain 团队开发，用于研究机器学习和深度神经网络 (DNN)，其通用性同样适用于其他各种领域。</p><h2 id="tensorflow-的工作原理" tabindex="-1"><a class="header-anchor" href="#tensorflow-的工作原理"><span>TensorFlow 的工作原理</span></a></h2><p>TensorFlow 工作流程由三个不同的部分定义，即数据预处理、构建模型和训练模型，从而进行预测。该框架将数据输入称为张量的多维数组，并以两种不同的方式执行。主要方法是构建一个计算图形来定义用于训练模型的数据流。第二种常用的更直观的方法是使用 Eager Execution，该方法遵循命令编程原则并立即评估操作。</p><p>使用 TensorFlow 架构，通常在台式电脑或数据中心完成训练。这两种情况下，均通过在 GPU 上放置张量来加快处理速度。然后，经过训练的模型可以在一系列平台上运行，从台式电脑到移动设备，然后一直到云端。</p><p>TensorFlow 还包含很多支持功能。例如，TensorBoard 允许用户以直观方式监控训练过程、底层计算图形和指标，以便调试运行以及评估模型性能。TensorBoard 是 Tensorflow 和 Keras 的统一可视化框架。</p><p>Keras 是在 TensorFlow 上运行的高级 API。Keras 通过提供用于构建常见用例模型的简化 API，进一步深化 TensorFlow 的抽象概念。API 背后的驱动理念是能够在更短的时间内将想法落实为结果。</p><h2 id="tensorflow-的优势" tabindex="-1"><a class="header-anchor" href="#tensorflow-的优势"><span>TensorFlow 的优势</span></a></h2><p>TensorFlow 可用于开发自然语言处理、图像识别、手写识别以及基于计算的不同模拟（例如偏微分方程）等各种任务模型。</p><p>TensorFlow 的主要优势在于其能够跨多个加速平台执行低级运算、自动计算梯度、生产级可扩展性和可互操作的图形导出。通过为 Keras 提供高级 API 和 Eager Execution，替代 TensorFlow 上的数据流范式，始终可以轻松便捷地编写代码。</p><p>作为 TensorFlow 的原始开发者，Google 仍然大力支持该库，并加快其发展速度。例如，Google 创建了一个在线中心，用于共享用户创建的许多不同模型。</p><p>时间：2017年</p><p>关键技术：自注意力机制与多头注意力机制的完美融合</p><p>处理数据：针对长序列数据展现卓越处理能力</p><p>应用场景：广泛应用于自然语言处理、机器翻译、文本生成等诸多领域</p><p>Transformer，作为一种基于自注意力机制的神经网络模型，凭借其独特的架构和机制，成为了深度学习领域的璀璨明星。其精妙之处在于由多个编码器和解码器共同构建的基本结构，编码器负责将输入的序列精妙地转换为向量表示，而解码器则负责将这一向量表示巧妙地还原为输出序列。</p><p>Transformer的创新之处在于引入了自注意力机制，这一机制赋予了模型捕捉序列中长距离依赖关系的非凡能力。它不再局限于传统的局部信息处理，而是能够洞察全局，把握整体，从而在处理长序列数据时表现出色。</p><p>在自然语言处理领域，Transformer以其卓越的性能赢得了广泛的赞誉和应用。无论是机器翻译中的精确翻译，还是文本生成中的流畅表达，Transformer都展现出了令人瞩目的成果。它的出现，无疑为自然语言处理领域的发展注入了新的活力。</p><p>从2017年Google Brain团队推出Transformer架构逐步取代长短期记忆（LSTM）等“循环神经网络（RNN）模型”成为首选模型，到后来首个线性时间序列架构Mamba推出又对Transformer架构构成挑战，大语言模型底层架构的迭代正在迅速改变人们对于AI的认知和理解。</p><p>Transformer架构是一种用于序列建模的深度学习架构，最初由Vaswani等人在2017年提出，并广泛应用于自然语言处理（NLP）任务中。它引入了自注意力机制（self-attention）来捕捉输入序列中元素之间的依赖关系，并在大规模数据集上取得了显著的性能提升。以下是Transformer架构的主要组成部分：</p><p>自注意力机制（Self-Attention）：自注意力机制是Transformer的核心组件之一。它允许模型在输入序列中的每个位置上根据其他位置的信息进行加权聚合。通过计算每个位置与其他位置之间的相对重要性，模型可以更好地理解序列中不同元素之间的依赖关系。</p><p>编码器（Encoder）：编码器是Transformer的基本模块，用于将输入序列转换为上下文感知的表示。编码器由多个相同的层堆叠而成，每个层都包含自注意力机制和前馈神经网络（Feed-Forward Network）。自注意力机制用于捕捉输入序列内部的依赖关系，而前馈神经网络则提供非线性变换和特征映射。</p><p>解码器（Decoder）：解码器也是由多个相同的层堆叠而成，与编码器类似，但还包括额外的自注意力机制层，用于对编码器的输出进行进一步的上下文感知。解码器常用于生成目标序列，如机器翻译任务中将源语言句子翻译成目标语言句子。</p><p>位置编码（Positional Encoding）：由于Transformer中没有使用递归或卷积操作，模型需要一种方式来处理输入序列中的位置信息。位置编码是一种将位置信息嵌入到输入序列中的方法，使模型能够区分不同位置的元素。通常使用正弦和余弦函数生成位置编码。</p><p>多头注意力机制（Multi-Head Attention）：为了增强模型的表达能力和建模能力，Transformer中的自注意力机制被扩展为多个并行的注意力头。每个注意力头可以关注序列中不同的相关性和特征，然后将它们的输出进行拼接或加权平均。</p><pre><code>Transformer架构的突出特点是其并行性和全局视野能力，允许模型在处理长序列任务时保持高效。它在许多NLP任务中取得了巨大成功，如机器翻译、文本生成、问答系统等。同时，Transformer的思想也被广泛应用于其他领域，如计算机视觉和语音处理，取得了显著的成效。
</code></pre><h2 id="生成三个向量-查询向量query-vec、键向量key-vec、值向量value-vec" tabindex="-1"><a class="header-anchor" href="#生成三个向量-查询向量query-vec、键向量key-vec、值向量value-vec"><span>生成三个向量：查询向量query-vec、键向量key-vec、值向量value-vec</span></a></h2><p>通过向量方式计算自注意力的第一步，就是从每个编码器的输入向量(即每个单词的词向量)生成三个向量：查询向量query-vec、键向量key-vec、值向量value-vec</p><p>查询向量、键向量、值向量这三个向量的维度在论文中设置的是64，在维度上比词嵌入向量更低，因为词嵌入和编码器的输入/输出向量的维度是512，但也不是必须比编码器输入输出的维数小，这样做主要是为了让后续多头注意力的计算更稳定 （在下文你会看到，transformer通过多头注意力机制multi headed attention，对每个512维的输入向量都设置了8个头，不同的头关注每个输入向量不同的部分，而每个头的维度则是：512/8 = 64，且再多说一句，也可以设置为2个头，不一定非得设置为8个头）</p><p>至于这三个向量的生成方法是把输入的向量分别乘以三个不同的权重矩阵WQ 𝑊 𝑄 、WK 𝑊 𝐾 、WV 𝑊 𝑉 ，得到Q、K、V，而这些权重矩阵是在模型训练阶段中训练出来的「对于权重矩阵WQ 𝑊 𝑄 /WK 𝑊 𝐾 /WV 𝑊 𝑉 如何训练出来的，还是标准老套路：先随机初始化，然后在损失函数中表示出来，最后通过反向传播不断优化学习得出，最终目标是最小化模型的预测误差，至于什么是反向传播，请参见参考文献17」</p><p>最终使得输入序列的每个单词各自创建一个查询向量、一个键向量和一个值向量</p><p>可能有的读者有疑问了，设置这三个向量的用意何在或有何深意，实际上</p><p>查询向量Query是当前单词的表示形式，用于对所有其他单词(key)进行评分，我们只需要关注当前正在处理的token的query 键向量Key可以看做是序列中所有单词的标签，是在我们找相关单词时候的对照物 值向量Value是单词的实际表示，一旦我们对每个单词的相关度打分之后，我们就要对value进行相加表示当前正在处理单词的value</p><p>第一次看到这里的朋友，可能会有疑问，正如知乎上有人问(https://www.zhihu.com/question/341222779?sort=created)：为什么Transformer 需要进行Multi-head Attention，即多头注意力机制？</p><p>叫TniL的答道：可以类比CNN中同时使用多个滤波器的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征/信息 且论文中是这么说的：Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. 关于different representation subspaces，举一个不一定妥帖的例子：当你浏览网页的时候，你可能在颜色方面更加关注深色的文字，而在字体方面会去注意大的、粗体的文字。这里的颜色和字体就是两个不同的表示子空间。同时关注颜色和字体，可以有效定位到网页中强调的内容。使用多头注意力，也就是综合利用各方面的信息/特征（毕竟，不同的角度有着不同的关注点） 叫LooperXX的则答道：在Transformer中使用的多头注意力出现前，基于各种层次的各种fancy的注意力计算方式，层出不穷。而Transformer的多头注意力借鉴了CNN中同一卷积层内使用多个卷积核的思想，原文中使用了 8 个 scaled dot-product attention ，在同一multi-head attention 层中，输入均为 KQV，同时进行注意力的计算，彼此之前参数不共享，最终将结果拼接起来，这样可以允许模型在不同的表示子空间里学习到相关的信息，在此之前的 A Structured Self-attentive Sentence Embedding 也有着类似的思想</p><p>简而言之，就是希望每个注意力头，只关注最终输出序列中一个子空间，互相独立，其核心思想在于，抽取到更加丰富的特征信息</p><h2 id="gpt-为何采用单向transformer" tabindex="-1"><a class="header-anchor" href="#gpt-为何采用单向transformer"><span>GPT：为何采用单向Transformer</span></a></h2><p>什么是单向Transformer？在Transformer的文章中，提到了Encoder与Decoder使用的Transformer Block是不同的。怎么个不同？通过本文第三部分对Transformer的介绍可得知：</p><p>Encoder因为要编码整个句子，所以每个词都需要考虑上下文的关系。所以每个词在计算的过程中都是可以看到句子中所有的词的； 但是Decoder与Seq2Seq中的解码器类似，每个词都只能看到前面词的状态，所以是一个单向的Self-Attention结构 换言之，在解码Decoder Block中，使用了Masked Self-Attention(所谓Masked，即遮蔽的意思)，即句子中的每个词，都只能对包括自己在内的前面所有词进行Attention，这就是单向Transformer。</p><p><strong>而GPT使用的Transformer结构就是将Encoder中的Self-Attention替换成了Masked Self-Attention，从而每个位置的词看不到后面的词</strong></p><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考</span></a></h2><p><a href="https://blog.csdn.net/sinat_37574187/article/details/140287124" target="_blank" rel="noopener noreferrer">对比参考</a></p><p><a href="https://blog.csdn.net/v_JULY_v/article/details/127411638" target="_blank" rel="noopener noreferrer">Transformer通俗笔记：从Word2Vec、Seq2Seq逐步理解到GPT、BERT</a> 国内外已经有很多不错的资料，比如国外作者Jay Alammar的<a href="https://jalammar.github.io/illustrated-transformer" target="_blank" rel="noopener noreferrer">一篇图解Transformer：The Illustrated Transformer</a>，再比如国内张俊林老师的这篇<a href="https://www.julyedu.com/questions/interview-detail?kp_id=30&amp;cate=NLP&amp;quesId=3008" target="_blank" rel="noopener noreferrer">《说说NLP中的预训练技术发展史：从Word Embedding到Bert模型》</a>。</p><h2 id="参考-1" tabindex="-1"><a class="header-anchor" href="#参考-1"><span>参考</span></a></h2><p><a href="https://arxiv.org/abs/1701.06538" target="_blank" rel="noopener noreferrer">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></p><p><a href="https://tensorflow.google.cn/?hl=zh-cn" target="_blank" rel="noopener noreferrer">TensorFlow</a></p><p><a href="https://tensorflow.google.cn/tensorboard?hl=da" target="_blank" rel="noopener noreferrer">TensorBoard</a></p>`,46)]))}const i=r(t,[["render",s],["__file","ih1sw59u.html.vue"]]),f=JSON.parse('{"path":"/llm/transformer/ih1sw59u.html","title":"Transformer 介绍","lang":"zh-CN","frontmatter":{"title":"Transformer 介绍","createTime":"2025/02/22 10:45:09","permalink":"/llm/transformer/ih1sw59u.html","watermark":true},"headers":[],"readingTime":{"minutes":11.3,"words":3391},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/llm/transformer/Transformer介绍.md","bulletin":false}');export{i as comp,f as data};
