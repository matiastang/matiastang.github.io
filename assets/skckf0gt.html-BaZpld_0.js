import{_ as s,c as n,a as t,o as e}from"./app-CS9K37Kg.js";const p={};function m(i,a){return e(),n("div",null,a[0]||(a[0]=[t('<h2 id="前向传播-forward-propagation" tabindex="-1"><a class="header-anchor" href="#前向传播-forward-propagation"><span>前向传播（Forward Propagation）</span></a></h2><p>输入到输出各层计算（如卷积、池化层等），产生输出并完成损失函数计算。</p><h2 id="反向传播-back-propagation" tabindex="-1"><a class="header-anchor" href="#反向传播-back-propagation"><span>反向传播（Back Propagation）</span></a></h2><p>由输出到输入反向完成整个模型中各层的权重和输出对损失函数的梯度求解。</p><h2 id="梯度更新-weight-update" tabindex="-1"><a class="header-anchor" href="#梯度更新-weight-update"><span>梯度更新（Weight Update）</span></a></h2><p>根据指定的指定学习率，对模型权重通过梯度下降算法完成权重的更新。</p><h2 id="梯度下降-gradient-descent" tabindex="-1"><a class="header-anchor" href="#梯度下降-gradient-descent"><span>梯度下降（Gradient Descent）</span></a></h2><p>梯度下降是一种优化算法，用于最小化损失函数。它通过计算损失函数的梯度，然后沿着梯度的反方向更新模型的参数，从而逐步逼近最优解。常见的梯度下降算法包括随机梯度下降（SGD）、批量梯度下降（BGD）和Adam等。</p><h2 id="学习率-learning-rate" tabindex="-1"><a class="header-anchor" href="#学习率-learning-rate"><span>学习率（Learning Rate）</span></a></h2><p>学习率是梯度下降算法中的一个超参数，用于控制模型参数更新的步长。学习率过大可能会导致模型参数更新不稳定，学习率过小可能会导致模型训练速度过慢。</p><h2 id="训练周期-epoch" tabindex="-1"><a class="header-anchor" href="#训练周期-epoch"><span>训练周期（Epoch）</span></a></h2><p>训练周期是指模型在所有训练数据上迭代一次的过程。在训练过程中，模型会多次迭代训练数据，直到达到预定的训练周期数或达到预定的性能指标。</p><h2 id="批量大小-batch-size" tabindex="-1"><a class="header-anchor" href="#批量大小-batch-size"><span>批量大小（Batch Size）</span></a></h2><p>批量大小是指在一次梯度下降迭代中使用的样本数量。批量大小会影响模型的训练速度和模型性能。较大的批量大小可以提供更稳定的梯度估计，但可能会增加计算成本。较小的批量大小可以提供更灵活的梯度估计，但可能会增加训练时间。</p><h2 id="步长-step" tabindex="-1"><a class="header-anchor" href="#步长-step"><span>步长（Step）</span></a></h2><p>步长是指模型参数更新的步长。步长会影响模型的训练速度和模型性能。较大的步长可能会导致模型参数更新不稳定，较小的步长可能会导致模型训练速度过慢。</p><h2 id="数据增强" tabindex="-1"><a class="header-anchor" href="#数据增强"><span>数据增强</span></a></h2><p>数据增强主要是因为一个是数据量太少，我们需要对数据做一些变化合成出来一些新的数据，让模型能够通过训练实现参数拟合，，另一个就是数据太”正常“了，没法应对各种情况，可以让模型更健壮。</p><p>一个数据增强的例子：单应性 (Homography) 变换，它是射影几何中的概念，又称为射影变换。它把一个射影平面上的点 (三维齐次矢量) 映射到另一个射影平面上，并且把直线映射为直线，具有保线性质，以上变换都可以通过矩阵乘法完成所有操作。简单的讲，包括的操作有：<strong>图像翻转、旋转、缩放、移动、上下偏移、缩放等等。</strong></p><p>许多深度学习框架（例如 PyTorch、Keras 和 Tensorflow）都提供了用于增强数据（主要是图像数据集）的功能。</p><h2 id="训练集-training-set" tabindex="-1"><a class="header-anchor" href="#训练集-training-set"><span>训练集（Training Set）</span></a></h2><p>训练集是指用于训练机器学习模型的样本数据集。训练集通常由大量的样本组成，用于训练模型以学习数据的特征和模式。</p><h2 id="验证集-validation-set" tabindex="-1"><a class="header-anchor" href="#验证集-validation-set"><span>验证集（Validation Set）</span></a></h2><p>验证集是指用于评估机器学习模型性能的样本数据集。验证集通常由一部分训练集数据组成，用于在训练过程中评估模型的性能，并调整模型的超参数。</p><h2 id="测试集-test-set" tabindex="-1"><a class="header-anchor" href="#测试集-test-set"><span>测试集（Test Set）</span></a></h2><p>测试集是指用于评估机器学习模型最终性能的样本数据集。测试集通常由与训练集和验证集不同的数据组成，用于在模型训练完成后评估模型的最终性能。</p><h2 id="欠拟合-underfitting" tabindex="-1"><a class="header-anchor" href="#欠拟合-underfitting"><span>欠拟合（Underfitting）</span></a></h2><p>欠拟合是指在训练过程中，<strong>模型无法很好地拟合训练数据，导致预测结果不准确</strong>。欠拟合通常发生在模型过于简单或特征不足的情况下。</p><h2 id="过拟合-overfitting" tabindex="-1"><a class="header-anchor" href="#过拟合-overfitting"><span>过拟合（Overfitting）</span></a></h2><p>过拟合是指在训练过程中，模型过于复杂，以至于：<strong>在训练数据上表现良好，但在测试数据上表现较差</strong>。过拟合通常发生在模型过于复杂或训练数据不足的情况下。</p><h2 id="损失函数-loss-function" tabindex="-1"><a class="header-anchor" href="#损失函数-loss-function"><span>损失函数（Loss Function）</span></a></h2><p>损失函数（Loss Function）是一种用于衡量模型预测结果与真实值之间差异的函数。它通常用于训练机器学习模型，以最小化损失函数的值。常见的损失函数包括均方误差（MSE）、交叉熵（Cross-Entropy）等。</p><h2 id="激活函数-activation-function" tabindex="-1"><a class="header-anchor" href="#激活函数-activation-function"><span>激活函数（Activation Function）</span></a></h2><p>激活函数（Activation Function）是一种在神经网络中用于引入非线性特性的函数。它们将输入信号转换为输出信号，使得神经网络能够学习复杂的非线性关系。常见的激活函数包括Sigmoid、ReLU、Tanh等。</p><h2 id="梯度爆炸" tabindex="-1"><a class="header-anchor" href="#梯度爆炸"><span>梯度爆炸</span></a></h2><p>梯度爆炸是指在训练神经网络时，梯度值变得非常大，导致模型参数更新过大，从而使得模型训练不稳定甚至无法收敛。梯度爆炸通常发生在深层神经网络中，特别是在使用梯度下降算法时。</p><h2 id="梯度消失" tabindex="-1"><a class="header-anchor" href="#梯度消失"><span>梯度消失</span></a></h2><p>梯度消失是指在训练神经网络时，梯度值变得非常小，导致模型参数更新过小，从而使得模型训练不稳定甚至无法收敛。梯度消失通常发生在深层神经网络中，特别是在使用梯度下降算法时。</p><h2 id="vector" tabindex="-1"><a class="header-anchor" href="#vector"><span>Vector</span></a></h2><p>向量（Vector）是一种数学对象，表示为有序的数字列表。向量可以表示空间中的点、方向、速度等。在机器学习和人工智能中，向量通常用于表示特征或数据。</p><h2 id="embedding" tabindex="-1"><a class="header-anchor" href="#embedding"><span>Embedding</span></a></h2><p>嵌入（Embedding）是一种将离散的元素（如单词、类别等）映射到连续的向量空间的技术。嵌入可以将离散的元素表示为向量，以便在机器学习和人工智能中使用。嵌入可以用于各种任务，如文本分类、情感分析、推荐系统等。</p><p>词嵌入（Word embedding）是自然语言处理（NLP）中语言模型与表征学习技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。</p><p><strong>embedding就是一组词，将输入的特性从多个维度进行了详细的描述</strong>，为了在机器学习里容易计算，组成了所谓的这个嵌入向量。</p><h3 id="token-embedding" tabindex="-1"><a class="header-anchor" href="#token-embedding"><span>Token Embedding</span></a></h3><p>单词 Embedding(TE)：Token Embedding可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。</p><h3 id="position-embedding" tabindex="-1"><a class="header-anchor" href="#position-embedding"><span>Position Embedding</span></a></h3><p>位置 Embedding(PE)：因为Transformer有全局视野，位置也是要考虑的，PE 的维度与单词 Embedding 是一样的。</p><p>PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，位置向量有多种表示方式。其中一种常见的是正弦 - 余弦位置编码（Sinusoidal Position Encoding）。</p><p>位置编码的公式如下（这个公式看着挺好的，比起单纯的数值，都不需要考虑位数的问题，容量无限扩展）：</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><msup><mn>10000</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE_{pos,2i} = sin(pos / 10000^{2i/d_{model}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">in</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><msup><mn>10000</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE_{pos,2i+1} = cos(pos / 10000^{2i/d_{model}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathnormal">cos</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p><h2 id="softmax" tabindex="-1"><a class="header-anchor" href="#softmax"><span>softmax</span></a></h2><p>softmax函数是一种用于多分类问题的激活函数。它将输入的数值转换为概率分布，使得所有输出的概率之和为1。softmax函数常用于神经网络的最后一层，用于输出每个类别的概率。</p><h2 id="cnn" tabindex="-1"><a class="header-anchor" href="#cnn"><span>CNN</span></a></h2><h2 id="rnn" tabindex="-1"><a class="header-anchor" href="#rnn"><span>RNN</span></a></h2><h2 id="seq2seq" tabindex="-1"><a class="header-anchor" href="#seq2seq"><span>Seq2Seq</span></a></h2><p>Seq2Seq（Sequence to Sequence）是一种神经网络模型，用于将一个序列（如文本、音频、图像等）转换为另一个序列。它由两个主要部分组成：编码器和解码器。编码器将输入序列编码为一个固定长度的向量，解码器则将该向量解码为输出序列。</p><h2 id="encoder-decoder" tabindex="-1"><a class="header-anchor" href="#encoder-decoder"><span>Encoder-Decoder</span></a></h2><p>Encoder-Decoder 是一种神经网络架构，用于将一个序列（如文本、音频、图像等）转换为另一个序列。它由两个主要部分组成：编码器和解码器。编码器将输入序列编码为一个固定长度的向量，解码器则将该向量解码为输出序列。</p><p>Seq2Seq 属于 Encoder-Decoder 的超集。<strong>Seq2Seq 更强调目的，Encoder-Decoder 更强调方法</strong>。</p><h2 id="attention" tabindex="-1"><a class="header-anchor" href="#attention"><span>Attention</span></a></h2><p>人类利用有限的注意力资源从大量信息中快速筛选出高价值信息，这是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。Attention从注意力模型的命名方式看，借鉴了人类的注意力机制。从本质上讲，<strong>注意力机制是一种资源分配机制</strong>。它通过计算权重来实现资源分配，信息筛选和整合的作用，能够动态地自适应调整权重。</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>注意力（Attention）本身并不是Transformer独有的，在RNN等其他算法里照样可以用。</p></div><h2 id="multi-head-attention" tabindex="-1"><a class="header-anchor" href="#multi-head-attention"><span>Multi head Attention</span></a></h2><p>多头注意力（Multi head Attention）是Transformer模型中的一个重要组成部分。它通过使用多个并行的注意力机制来增强模型的表达能力。多头注意力机制将输入序列分解为多个子序列，并为每个子序列计算注意力权重。这些注意力权重可以捕捉到输入序列中的不同特征，从而提高模型的性能。不同的Attention侧重点可能略有不同，这样给了模型更大的容量。</p><p>不同Attention是可以并行计算的，不需要算完一个再算下一个，非常符合GPU并行计算的理念，当然逻辑上不同的Attention是分开来看的，它们可以针对不同观点场景。</p><p>实际上处理的时候不需要将多个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">W_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">W_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>几个矩阵分开，多头注意力下，同一个矩阵其实是拼接在一起的，没有必要分开，所以，又引出了一个思考，关于Embedding Size的问题，是需要考虑多头机制的。</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo>=</mo><mi>n</mi><mi>o</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>s</mi><mo>∗</mo><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">embedding size = nos of heads * query size</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">mb</span><span class="mord mathnormal">e</span><span class="mord mathnormal">dd</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">n</span><span class="mord mathnormal">oso</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mord mathnormal">u</span><span class="mord mathnormal">erys</span><span class="mord mathnormal">i</span><span class="mord mathnormal">ze</span></span></span></span></p><h2 id="masking" tabindex="-1"><a class="header-anchor" href="#masking"><span>Masking</span></a></h2><p>掩码（masking）是一种用于处理序列数据的技巧，特别是在自然语言处理（NLP）中。它用于掩盖序列中的某些元素，以便模型在处理这些元素时不会受到干扰。例如，在机器翻译任务中，掩码可以用于掩盖目标序列中的未知词，以便模型在生成翻译时不会受到未知词的影响。</p><p>比如你的输入是带留白的（这里的PAD是留白填充的意思），因为留白不应该做Attention，于是我们把它屏蔽掉。</p><p>除了这种情况以外，比如为了可以同时输入多个序列，达到并行的效果，但是其实它们是有因果关系的，为了避免输出能看到未来，所以要做下屏蔽，免得输出在做当前输出时能看到未来的输入，造成混乱。</p><p><strong>总之一句话，注意力掩码本质上是一种阻止模型看我们不想让它看的信息的方法。</strong></p><h2 id="幻觉" tabindex="-1"><a class="header-anchor" href="#幻觉"><span>幻觉</span></a></h2><p>在训练神经网络时，可能会出现一种现象，即模型在训练过程中逐渐偏离其目标函数，导致模型性能下降。这种现象被称为“幻觉”（Hallucination）。幻觉通常发生在模型训练过程中，特别是在使用梯度下降算法时。</p><p>幻觉也出现在推理阶段，即模型在生成输出时可能会生成一些不存在的信息。即我们常说的<strong>一本正经的胡说八道</strong></p><h2 id="transformer" tabindex="-1"><a class="header-anchor" href="#transformer"><span>Transformer</span></a></h2><h2 id="pytorch" tabindex="-1"><a class="header-anchor" href="#pytorch"><span>PyTorch</span></a></h2><h2 id="动态图" tabindex="-1"><a class="header-anchor" href="#动态图"><span>动态图</span></a></h2><h2 id="静态图" tabindex="-1"><a class="header-anchor" href="#静态图"><span>静态图</span></a></h2><h2 id="深度学习" tabindex="-1"><a class="header-anchor" href="#深度学习"><span>深度学习</span></a></h2><h2 id="机器学习" tabindex="-1"><a class="header-anchor" href="#机器学习"><span>机器学习</span></a></h2><h2 id="人工智能" tabindex="-1"><a class="header-anchor" href="#人工智能"><span>人工智能</span></a></h2><h2 id="迁移学习" tabindex="-1"><a class="header-anchor" href="#迁移学习"><span>迁移学习</span></a></h2><p>取一个预先训练好的模型，并将它的末端切掉，而只使用原模型的前面几层作为头层，再在其下继续构建我们自己的新层—— 经过实践这种做法是可行的，这个叫迁移学习。迁移学习很重要，这是我们开发各种类型模型的基础。</p><p><a href="https://www.tensorflow.org/tutorials/generative/deepdream" target="_blank" rel="noopener noreferrer">tensorflow deepdream</a></p><h2 id="强化学习" tabindex="-1"><a class="header-anchor" href="#强化学习"><span>强化学习</span></a></h2><h2 id="无监督学习" tabindex="-1"><a class="header-anchor" href="#无监督学习"><span>无监督学习</span></a></h2><h2 id="监督学习" tabindex="-1"><a class="header-anchor" href="#监督学习"><span>监督学习</span></a></h2><h2 id="半监督学习" tabindex="-1"><a class="header-anchor" href="#半监督学习"><span>半监督学习</span></a></h2><h2 id="自监督学习" tabindex="-1"><a class="header-anchor" href="#自监督学习"><span>自监督学习</span></a></h2><h2 id="强化学习-1" tabindex="-1"><a class="header-anchor" href="#强化学习-1"><span>强化学习</span></a></h2><h2 id="生成对抗网络" tabindex="-1"><a class="header-anchor" href="#生成对抗网络"><span>生成对抗网络</span></a></h2><h2 id="深度强化学习" tabindex="-1"><a class="header-anchor" href="#深度强化学习"><span>深度强化学习</span></a></h2><h2 id="多模态" tabindex="-1"><a class="header-anchor" href="#多模态"><span>多模态</span></a></h2><h2 id="多模态预训练" tabindex="-1"><a class="header-anchor" href="#多模态预训练"><span>多模态预训练</span></a></h2><h2 id="多模态大模型" tabindex="-1"><a class="header-anchor" href="#多模态大模型"><span>多模态大模型</span></a></h2><h2 id="模型量化" tabindex="-1"><a class="header-anchor" href="#模型量化"><span>模型量化</span></a></h2><p>模型量化是指将模型的参数从浮点数转换为定点数的过程。定点数是一种表示实数的二进制方法，它使用固定的小数位数和整数位数来表示实数。模型量化可以减少模型的存储空间和计算资源需求，从而提高模型的运行速度和效率。</p><h2 id="cnn-1" tabindex="-1"><a class="header-anchor" href="#cnn-1"><span>CNN</span></a></h2><h2 id="rnn-1" tabindex="-1"><a class="header-anchor" href="#rnn-1"><span>RNN</span></a></h2><h2 id="注意力机制" tabindex="-1"><a class="header-anchor" href="#注意力机制"><span>注意力机制</span></a></h2><h2 id="计算机视觉-computer-vision-cv" tabindex="-1"><a class="header-anchor" href="#计算机视觉-computer-vision-cv"><span>计算机视觉(Computer Vision, CV)</span></a></h2><h2 id="生成时对抗神经网络gan" tabindex="-1"><a class="header-anchor" href="#生成时对抗神经网络gan"><span>生成时对抗神经网络GAN</span></a></h2><h2 id="lstm" tabindex="-1"><a class="header-anchor" href="#lstm"><span>LSTM</span></a></h2>',106)]))}const h=s(p,[["render",m],["__file","skckf0gt.html.vue"]]),l=JSON.parse('{"path":"/ai/skckf0gt.html","title":"基本概念","lang":"zh-CN","frontmatter":{"title":"基本概念","createTime":"2025/03/25 10:32:30","permalink":"/ai/skckf0gt.html","watermark":true},"headers":[],"readingTime":{"minutes":11.18,"words":3355},"git":{"updatedTime":1755670488000,"contributors":[{"name":"唐道勇","username":"唐道勇","email":"matias@tangdaoyongdeMacBook-Pro.local","commits":1,"avatar":"https://avatars.githubusercontent.com/唐道勇?v=4","url":"https://github.com/唐道勇"}]},"filePathRelative":"notes/ai/基本概念.md","bulletin":false}');export{h as comp,l as data};
